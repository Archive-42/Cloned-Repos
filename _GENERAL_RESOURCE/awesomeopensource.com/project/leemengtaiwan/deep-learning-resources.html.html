<!DOCTYPE html>
<html>
  <head>
<script>window.NREUM||(NREUM={});NREUM.info={"beacon":"bam-cell.nr-data.net","errorBeacon":"bam-cell.nr-data.net","licenseKey":"98ebaa2fbd","applicationID":"233526624","transactionName":"c11aERNaXltSERdDR19YUQYVGltZUwZA","queueTime":3,"applicationTime":187,"agent":""}</script>
<script>(window.NREUM||(NREUM={})).loader_config={licenseKey:"98ebaa2fbd",applicationID:"233526624"};window.NREUM||(NREUM={}),__nr_require=function(e,t,n){function r(n){if(!t[n]){var i=t[n]={exports:{}};e[n][0].call(i.exports,function(t){var i=e[n][1][t];return r(i||t)},i,i.exports)}return t[n].exports}if("function"==typeof __nr_require)return __nr_require;for(var i=0;i<n.length;i++)r(n[i]);return r}({1:[function(e,t,n){function r(){}function i(e,t,n){return function(){return o(e,[u.now()].concat(c(arguments)),t?null:this,n),t?void 0:this}}var o=e("handle"),a=e(6),c=e(7),f=e("ee").get("tracer"),u=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var d=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],p="api-",l=p+"ixn-";a(d,function(e,t){s[t]=i(p+t,!0,"api")}),s.addPageAction=i(p+"addPageAction",!0),s.setCurrentRouteName=i(p+"routeName",!0),t.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,t){var n={},r=this,i="function"==typeof t;return o(l+"tracer",[u.now(),e,n],r),function(){if(f.emit((i?"":"no-")+"fn-start",[u.now(),r,i],n),i)try{return t.apply(this,arguments)}catch(e){throw f.emit("fn-err",[arguments,this,e],n),e}finally{f.emit("fn-end",[u.now()],n)}}}};a("actionText,setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,t){m[t]=i(l+t)}),newrelic.noticeError=function(e,t){"string"==typeof e&&(e=new Error(e)),o("err",[e,u.now(),!1,t])}},{}],2:[function(e,t,n){function r(){return c.exists&&performance.now?Math.round(performance.now()):(o=Math.max((new Date).getTime(),o))-a}function i(){return o}var o=(new Date).getTime(),a=o,c=e(8);t.exports=r,t.exports.offset=a,t.exports.getLastTimestamp=i},{}],3:[function(e,t,n){function r(e,t){var n=e.getEntries();n.forEach(function(e){"first-paint"===e.name?d("timing",["fp",Math.floor(e.startTime)]):"first-contentful-paint"===e.name&&d("timing",["fcp",Math.floor(e.startTime)])})}function i(e,t){var n=e.getEntries();n.length>0&&d("lcp",[n[n.length-1]])}function o(e){e.getEntries().forEach(function(e){e.hadRecentInput||d("cls",[e])})}function a(e){if(e instanceof m&&!g){var t=Math.round(e.timeStamp),n={type:e.type};t<=p.now()?n.fid=p.now()-t:t>p.offset&&t<=Date.now()?(t-=p.offset,n.fid=p.now()-t):t=p.now(),g=!0,d("timing",["fi",t,n])}}function c(e){d("pageHide",[p.now(),e])}if(!("init"in NREUM&&"page_view_timing"in NREUM.init&&"enabled"in NREUM.init.page_view_timing&&NREUM.init.page_view_timing.enabled===!1)){var f,u,s,d=e("handle"),p=e("loader"),l=e(5),m=NREUM.o.EV;if("PerformanceObserver"in window&&"function"==typeof window.PerformanceObserver){f=new PerformanceObserver(r);try{f.observe({entryTypes:["paint"]})}catch(v){}u=new PerformanceObserver(i);try{u.observe({entryTypes:["largest-contentful-paint"]})}catch(v){}s=new PerformanceObserver(o);try{s.observe({type:"layout-shift",buffered:!0})}catch(v){}}if("addEventListener"in document){var g=!1,w=["click","keydown","mousedown","pointerdown","touchstart"];w.forEach(function(e){document.addEventListener(e,a,!1)})}l(c)}},{}],4:[function(e,t,n){function r(e,t){if(!i)return!1;if(e!==i)return!1;if(!t)return!0;if(!o)return!1;for(var n=o.split("."),r=t.split("."),a=0;a<r.length;a++)if(r[a]!==n[a])return!1;return!0}var i=null,o=null,a=/Version\/(\S+)\s+Safari/;if(navigator.userAgent){var c=navigator.userAgent,f=c.match(a);f&&c.indexOf("Chrome")===-1&&c.indexOf("Chromium")===-1&&(i="Safari",o=f[1])}t.exports={agent:i,version:o,match:r}},{}],5:[function(e,t,n){function r(e){function t(){e(a&&document[a]?document[a]:document[i]?"hidden":"visible")}"addEventListener"in document&&o&&document.addEventListener(o,t,!1)}t.exports=r;var i,o,a;"undefined"!=typeof document.hidden?(i="hidden",o="visibilitychange",a="visibilityState"):"undefined"!=typeof document.msHidden?(i="msHidden",o="msvisibilitychange"):"undefined"!=typeof document.webkitHidden&&(i="webkitHidden",o="webkitvisibilitychange",a="webkitVisibilityState")},{}],6:[function(e,t,n){function r(e,t){var n=[],r="",o=0;for(r in e)i.call(e,r)&&(n[o]=t(r,e[r]),o+=1);return n}var i=Object.prototype.hasOwnProperty;t.exports=r},{}],7:[function(e,t,n){function r(e,t,n){t||(t=0),"undefined"==typeof n&&(n=e?e.length:0);for(var r=-1,i=n-t||0,o=Array(i<0?0:i);++r<i;)o[r]=e[t+r];return o}t.exports=r},{}],8:[function(e,t,n){t.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,t,n){function r(){}function i(e){function t(e){return e&&e instanceof r?e:e?u(e,f,a):a()}function n(n,r,i,o,a){if(a!==!1&&(a=!0),!l.aborted||o){e&&a&&e(n,r,i);for(var c=t(i),f=v(n),u=f.length,s=0;s<u;s++)f[s].apply(c,r);var p=d[h[n]];return p&&p.push([b,n,r,c]),c}}function o(e,t){y[e]=v(e).concat(t)}function m(e,t){var n=y[e];if(n)for(var r=0;r<n.length;r++)n[r]===t&&n.splice(r,1)}function v(e){return y[e]||[]}function g(e){return p[e]=p[e]||i(n)}function w(e,t){s(e,function(e,n){t=t||"feature",h[n]=t,t in d||(d[t]=[])})}var y={},h={},b={on:o,addEventListener:o,removeEventListener:m,emit:n,get:g,listeners:v,context:t,buffer:w,abort:c,aborted:!1};return b}function o(e){return u(e,f,a)}function a(){return new r}function c(){(d.api||d.feature)&&(l.aborted=!0,d=l.backlog={})}var f="nr@context",u=e("gos"),s=e(6),d={},p={},l=t.exports=i();t.exports.getOrSetContext=o,l.backlog=d},{}],gos:[function(e,t,n){function r(e,t,n){if(i.call(e,t))return e[t];var r=n();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,t,{value:r,writable:!0,enumerable:!1}),r}catch(o){}return e[t]=r,r}var i=Object.prototype.hasOwnProperty;t.exports=r},{}],handle:[function(e,t,n){function r(e,t,n,r){i.buffer([e],r),i.emit(e,t,n)}var i=e("ee").get("handle");t.exports=r,r.ee=i},{}],id:[function(e,t,n){function r(e){var t=typeof e;return!e||"object"!==t&&"function"!==t?-1:e===window?0:a(e,o,function(){return i++})}var i=1,o="nr@id",a=e("gos");t.exports=r},{}],loader:[function(e,t,n){function r(){if(!x++){var e=b.info=NREUM.info,t=p.getElementsByTagName("script")[0];if(setTimeout(u.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&t))return u.abort();f(y,function(t,n){e[t]||(e[t]=n)});var n=a();c("mark",["onload",n+b.offset],null,"api"),c("timing",["load",n]);var r=p.createElement("script");r.src="https://"+e.agent,t.parentNode.insertBefore(r,t)}}function i(){"complete"===p.readyState&&o()}function o(){c("mark",["domContent",a()+b.offset],null,"api")}var a=e(2),c=e("handle"),f=e(6),u=e("ee"),s=e(4),d=window,p=d.document,l="addEventListener",m="attachEvent",v=d.XMLHttpRequest,g=v&&v.prototype;NREUM.o={ST:setTimeout,SI:d.setImmediate,CT:clearTimeout,XHR:v,REQ:d.Request,EV:d.Event,PR:d.Promise,MO:d.MutationObserver};var w=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1194.min.js"},h=v&&g&&g[l]&&!/CriOS/.test(navigator.userAgent),b=t.exports={offset:a.getLastTimestamp(),now:a,origin:w,features:{},xhrWrappable:h,userAgent:s};e(1),e(3),p[l]?(p[l]("DOMContentLoaded",o,!1),d[l]("load",r,!1)):(p[m]("onreadystatechange",i),d[m]("onload",r)),c("mark",["firstbyte",a.getLastTimestamp()],null,"api");var x=0},{}],"wrap-function":[function(e,t,n){function r(e,t){function n(t,n,r,f,u){function nrWrapper(){var o,a,s,p;try{a=this,o=d(arguments),s="function"==typeof r?r(o,a):r||{}}catch(l){i([l,"",[o,a,f],s],e)}c(n+"start",[o,a,f],s,u);try{return p=t.apply(a,o)}catch(m){throw c(n+"err",[o,a,m],s,u),m}finally{c(n+"end",[o,a,p],s,u)}}return a(t)?t:(n||(n=""),nrWrapper[p]=t,o(t,nrWrapper,e),nrWrapper)}function r(e,t,r,i,o){r||(r="");var c,f,u,s="-"===r.charAt(0);for(u=0;u<t.length;u++)f=t[u],c=e[f],a(c)||(e[f]=n(c,s?f+r:r,i,f,o))}function c(n,r,o,a){if(!m||t){var c=m;m=!0;try{e.emit(n,r,o,t,a)}catch(f){i([f,n,r,o],e)}m=c}}return e||(e=s),n.inPlace=r,n.flag=p,n}function i(e,t){t||(t=s);try{t.emit("internal-error",e)}catch(n){}}function o(e,t,n){if(Object.defineProperty&&Object.keys)try{var r=Object.keys(e);return r.forEach(function(n){Object.defineProperty(t,n,{get:function(){return e[n]},set:function(t){return e[n]=t,t}})}),t}catch(o){i([o],n)}for(var a in e)l.call(e,a)&&(t[a]=e[a]);return t}function a(e){return!(e&&e instanceof Function&&e.apply&&!e[p])}function c(e,t){var n=t(e);return n[p]=e,o(e,n,s),n}function f(e,t,n){var r=e[t];e[t]=c(r,n)}function u(){for(var e=arguments.length,t=new Array(e),n=0;n<e;++n)t[n]=arguments[n];return t}var s=e("ee"),d=e(7),p="nr@original",l=Object.prototype.hasOwnProperty,m=!1;t.exports=r,t.exports.wrapFunction=c,t.exports.wrapInPlace=f,t.exports.argsToArray=u},{}]},{},["loader"]);</script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-6242674-23"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-6242674-23', { 'optimize_id': 'GTM-T48MRN9'});
    </script>

    <title>Deep Learning Resources</title>
<meta name="description" content="由淺入深的深度學習資源 Collection of deep learning materials for everyone">
<meta name="robots" content="index, follow">
<link rel="canonical" href="https://awesomeopensource.com/project/leemengtaiwan/deep-learning-resources">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>

      /* copy template below */
      /* ------------------- */
      /* ----------------- */
      /* ----------------- */

      /*ipad and below */


      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {

      }

      /* end template copy */
      /* ----------------- */
      /* ----------------- */
      /* ----------------- */

      /*ipad and below */
      .show_on_phones {
        display: block;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .show_on_phones {
          display: none;
        }
      }

      .twenty_px_padding_top {
        padding-top: 20px;
      }

      /*ipad and below */
      .projects_category_explorer_left_column {
        max-width: 0;
        margin-right: 0;
        display: none;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .projects_category_explorer_left_column {
          min-width: 20%;
          margin-right: 5%;
          display: block;
        }
      }

      /*ipad and below */
      .projects_list_right_column {
        min-width: 100%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .projects_list_right_column {
          min-width: 75%;
        }
      }

      .display_flex {
        display: flex;
      }

      .margin_bottom_15 {
        margin-bottom: 15px;
      }

      /*ipad and below */
      .image-gallery-image {
        width: 100% !important;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .image-gallery-image {
          width: 1280px !important;
        }
      }

      /*ipad and below */
      .aos_sponsorship_link {
        font-size: 14px;
        font-weight: bold;
        color: blue;
        text-decoration: underline;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_sponsorship_link {
          font-size: 18px;
          font-weight: bold;
          color: blue;
          text-decoration: underline;
        }
      }

      /*ipad and below */
      .aos_desktop_right_ads_container {
        display: none;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_desktop_right_ads_container {
          display: block;
        }
      }

      /*ipad and below */
      .aos_project_screen_container {
        margin-left: 5%;
        margin-right: 5%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_screen_container {
          margin-left: 5%;
          margin-right: 0;
          display: flex;
        }
      }

      /*ipad and below */
      .aos_project_inner_screen_container {
        width: 100%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_inner_screen_container {
          width: 95%;
        }
      }

      /*ipad and below
      .aos_license_text_metadata {
        margin-top: 0 !important;
      }

      everything larger than ipad
      @media screen and (min-width: 1024px) {
        .aos_license_text_metadata {
          margin-top: 0 !important;
        }
      }*/

      /*ipad and below */
      .projects_list_ad_slot_1 {
        height: fit-content;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .projects_list_ad_slot_1 {
          height: fit-content;
        }
      }

      /*ipad and below */
      .projects_list_ad_slot_2 {
        height: fit-content;
        margin-top: -34px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .projects_list_ad_slot_2 {
          height: fit-content;
          margin-top: 0;
        }
      }

      /*ipad and below */
      .project_page_ad_slot_1 {
        display: block;
        height: fit-content;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .project_page_ad_slot_1 {
          display: block;
          height: fit-content;
        }
      }

      /*ipad and below */
      .project_page_ad_slot_2 {
        display: block;
        height: fit-content;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .project_page_ad_slot_2 {
          display: block;
          height: fit-content;
        }
      }

      /*ipad and below */
      .aos_category_explorer_category_container {
        background-color: white;
        padding: 10px;
        margin-bottom: 0px;
        border-bottom: 1px solid lightgray;
        border-left: 1px solid lightgray;
        border-right: 1px solid lightgray;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_category_explorer_category_container {
          background-color: white;
          padding: 10px;
          margin-bottom: 0px;
          border-bottom: 1px solid lightgray;
          border-left: 1px solid lightgray;
          border-right: 1px solid lightgray;
        }
      }

      .aos_category_explorer_category_container:first-child {
        border-top: 1px solid lightgray;
      }

      /*ipad and below */
      .aos_topic_count {
        font-size: 10px;
        color: #4f4f4f;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_topic_count {
          font-size: 16px;
          color: #4f4f4f;
        }
      }

      /*ipad and below */
      .aos_topic_icon {
        display: none;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_topic_icon {
          font-size: 14px;
          display: inline-block;
        }
      }

      /*ipad and below */
      .aos_project_count {
        color: #4f4f4f;
        font-size: 14px;
        font-weight: bold;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_count {
          font-size: 14px;
          font-weight: bold;
        }
      }

      /*ipad and below */
      .aos_project_star {
        font-size: 14px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_star {
          font-size: 14px;
          /*margin-right: 7px;*/
        }
      }

      /*ipad and below */
      .aos_project_metadata_trademark_notice {
        font-size: 8px;
        color: gray;
        margin-top: 10px;
        border-bottom: 1px solid lightgray;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_metadata_trademark_notice {
          font-size: 12px;
          color: gray;
          margin-top: 20px;
          border-bottom: 1px solid lightgray;
        }
      }

      /*ipad and below */
      .aos_project_metadata_description {
        margin-bottom: 10px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_metadata_description {
          margin-bottom: 5px;
        }
      }

      .aos_project_metadata {
        display: inline-block;
        margin-bottom: 10px;
        width: 100%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_metadata {
          display: inline-block;
          margin-bottom: 10px;
          width: 100%;
        }
      }

      /*ipad and below */
      .aos_project_metadata_content {
        /*border-bottom: 1px solid lightgray;*/
        font-size: 16px;
        color: #000000;
        font-weight: 300;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_metadata_content {
          /*border-bottom: 1px solid lightgray;*/
          font-size: 16px;
          color: #000000;
          font-weight: 300;
        }
      }

      /*ipad and below */
      .aos_project_metadata_header {
        color: #808080;
        margin-bottom: 10px;
        margin-top: 10px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_metadata_header {
          color: #808080;
          margin-bottom: 10px;
          margin-top: 10px;
          font-size: 16px;
        }
      }

      aos_project_metadata_header:hover {
        color: #808080;
      }

      /*ipad and below */
      .aos_project_metadata_title {
        font-size: 20px;
        margin-bottom: 5px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_metadata_title {
          font-size: 30px;
          margin-bottom: 10px;
        }
      }

      /*ipad and below */
      .aos_app_container {
        display: flex;
        flex-direction: row;
        width: 100%;
        margin-bottom: 2px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_app_container {
          display: flex;
          flex-direction: row;
          width: 100%;
          margin-bottom: 0;
        }
      }

      @media screen and (min-width: 1025px) {
        .aos_desktop_ad_spacer {
          margin-top: 5px;
          margin-bottom: 5px;
        }
      }

      /*ipad and below */
      .aos_show_ad_on_mobile {
        display: none;
      }

      @media screen and (max-width: 1024px) {
        .aos_show_ad_on_mobile {
          /*display: inline-block;*/
          display: block; /*adsense recommended*/
        }
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1025px) {
        .aos_show_ad_on_desktop {
          /*display: inline-block;*/
          display: block; /*adsense recommended*/
        }
      }

      /*ipad and below */
      @media screen and (max-width: 1024px) {
        .aos_show_ad_on_desktop {
          display: none;
        }
      }

      /*ipad and below */
      .aos_filter_topics_input {
        font-size: 16px;
        padding: 10px;
        border: 1px solid lightgray;
        background-color: white;
        border-radius: 10px;
        box-shadow: 1px 1px 5px lightgrey;
        margin-bottom: 20px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_filter_topics_input {
          font-size: 20px;
          padding: 10px;
          border: 1px solid lightgray;
          background-color: white;
          border-radius: 10px;
          box-shadow: 1px 1px 5px lightgrey;
          margin-bottom: 20px;
        }
      }

      /* ipad and below */
      .aos_topics_project {
        font-family: helvetica, arial, sans-serif;
        font-size: 12px;
        padding: 0;
        display: flex;
        flex-direction: column;
        align-items: flex-start;
        width: 100%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_topics_project {
          font-family: helvetica, arial, sans-serif;
          margin-bottom: 0;
          font-size: 20px;
          border-radius: 10px;
          padding: 0;
          display: flex;
          flex-direction: column;
          align-items: flex-start;
          width: 100%;
        }
      }

      /* ipad and below
      .aos_topics_sidebar {
        font-family: helvetica, arial, sans-serif;
        font-size: 12px;
        margin-left: 2%;
        margin-right: 2%;
        width: 94%;
        padding: 0;
        display: flex;
        flex-direction: column;
      }

       everything larger than ipad
      @media screen and (min-width: 1024px) {
        .aos_topics_sidebar {
          font-family: helvetica, arial, sans-serif;
          margin-left: 5%;
          margin-right: 2%;
          width: 30%;
          margin-bottom: 0;
          font-size: 20px;
          border-radius: 10px;
          padding: 0;
          display: flex;
          flex-direction: column;
        }
      }*/

      /*ipad and below */
      .aos_pill_box {
        display: flex;
        color: black;
        border-radius: 5px;
        padding: 2px;
        margin: 2px;
        align-items: center;
        border-color: black;
        background-color: white;
        font-size: 12px;
        border: 1px solid black;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_pill_box {
          display: flex;
          color: black;
          border-radius: 5px;
          padding: 5px;
          margin: 10px;
          align-items: center;
          border: 0;
          background-color: white;
          box-shadow: 1px 1px 10px 1px rgba(0,0,0,0.25);
          font-size: 16px;
        }
      }

      /*ipad and below */
      .aos_selected_topics_text_container {
        display: none;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_selected_topics_text_container {
          margin-left: 25px;
          margin-right: 10px;
          margin-bottom: 10px;
          margin-top: 10px;
          padding: 5px;
          font-weight: bold;
          font-size: 20px;
          white-space: nowrap;
          display: inline-block;
        }
      }

      /*ipad and below */
      .aos_pagenavigator_links_container {
        display: flex;
        justify-content: space-around;
        font-size: 12px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_pagenavigator_links_container {
          display: flex;
          justify-content: space-around;
          font-size: 16px;
        }
      }

      /*ipad and below */
      .aos_project_container {
        padding-bottom: 20px;
        padding-left: 0;
        padding-right: 0;
        padding-top: 5px;
        width: 100%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_container {
          /*border-radius: 10px;*/
          padding-bottom: 20px;
          padding-left: 0;
          padding-right: 0;
          padding-top: 5px;
          width: 100%;
        }
      }

      /*ipad and below */
      .aos_project_description {
        font-size: 16px;
        width: 100%;
        font-weight: 300; /* match adsense */
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_description {
          font-size: 16px;
          font-weight: 300; /* match adsense */
        }
      }

      /*ipad and below */
      .aos_project_title {
        font-size: 20px;
        margin-bottom: 2px;
        font-weight: 400; /* match adsense */
        width: 100%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_project_title {
          font-size: 20px;
          margin-bottom: 2px;
          font-weight: 400; /* match adsense */
        }
      }

      .aos_project_title a :hover {
        text-decoration: none !important;
      }

      .aos_no_underline:hover {
        text-decoration: none !important;
      }

      .aos_grey_link:hover {
        color: #808080 !important;
      }

      .aos_grey_link {
        color: #808080 !important;
      }

      /*ipad and below */
      .category_topic_filter_container {
        width: 94%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .category_topic_filter_container {
          width: 94%;
        }
      }

      /*ipad and below */
      .aos_projects_container {
        font-family: helvetica, arial, sans-serif;
        margin-right: 2%;
        width: 96%; /* won't insert an ad this small (68%), but bigger and it overflows to the right with some topics that are long */
        font-size: 14px;
        margin-bottom: 5px;
        margin-left: 2%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_projects_container {
          font-family: helvetica, arial, sans-serif;
          margin-right: 0;
          width: 90%; /* was 53 before removing right margin spacer / ad container */
          font-size: 18px;
          margin-bottom: 5px;
          min-width: 800px;
          margin-left: 5%;
        }
      }

      /*ipad and below */
      .aos_pagenavigator_container {
        display: flex;
        justify-content: center;
        font-size: 9px;
        margin-bottom: 0;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_pagenavigator_container {
          display: flex;
          justify-content: center;
          font-size: 16px;
          margin-bottom: 10px;
        }
      }

      /*ipad and below */
      .aos_header_title_container_ipad_and_smaller {
        font-size: 12px;
        font-weight: bold;
        display: flex;
        align-items: center;
        justify-content: center;
        width: 134px;
        white-space: nowrap;
        flex-direction: column;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_header_title_container_ipad_and_smaller {
          display: none;
        }
      }


      /*ipad and below */
      .aos_header_title_container_larger_than_ipad {
        display: none;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_header_title_container_larger_than_ipad {
          display: block;
        }
      }

      /*ipad and below */
      .aos_logo_image {
        height: 25px;
        width: 25px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_logo_image {
          height: 50px;
          width: 50px;
        }
      }

      /*ipad and below */
      .aos_fixed_footer_container {
        display: flex;
        margin-left: 0;
        margin-right: 0;
        margin-top: 2px;
        width: 100%;
        justify-content: center;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_fixed_footer_container {
          display: flex;
          margin-left: 5%;
          margin-right: 20%;
          margin-top: 5px;
          margin-bottom: 5px;
          width: 75%;
          justify-content: center;
        }
      }

      /*ipad and below */
      .aos_fixed_header_container {
        margin-left: 2%;
        margin-right: 2%;
        margin-top: 4px;
        margin-bottom: 2px;
        width: 96%;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_fixed_header_container {
          margin-left: 5%;
          margin-right: 20%;
          margin-top: 5px;
          margin-bottom: 5px;
          width: 75%;
        }
      }

      /*ipad and below */
      body {
        margin-top: 0;
        margin-right: 0;
        margin-bottom: 0;
        margin-left: 0;
        padding: 0
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        body {
          margin-top: 0;
          margin-right: 0;
          margin-bottom: 0;
          margin-left: 0;
          padding: 0
        }
      }

      /* ipad and below */
      .aos_topics_sidebar_link {
        width: 100%;
        margin-bottom: 10px;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
        font-size: 14px;
      }

      /* everything larger than ipad */
      @media screen and (min-width: 1024px) {
        .aos_topics_sidebar_link {
          width: 100%;
          margin-bottom: 10px;
          font-size: 18px;
        }
      }

      html {
        margin: 0;
        padding: 0
      }

      a {
        color: #000000;
        text-decoration: none;
      }

      a:visited {
        text-decoration: none;
      }

      a:hover {
        /*color: #0366d6;*/
        color: #000000; /* match adsense */
        text-decoration: underline;
      }

      .AOS_Pill:hover {
        text-decoration: none;
      }

      /* MIT licensed github-markdown.css */
      /* https://github.com/sindresorhus/github-markdown-css */
      @font-face {
        font-family: octicons-link;
        src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
      }
      .markdown-body {
        -ms-text-size-adjust: 100%;
        -webkit-text-size-adjust: 100%;
        line-height: 1.5;
        color: #24292e;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
        font-size: 16px;
        line-height: 1.5;
        word-wrap: break-word;
      }
      .markdown-body .pl-c {
        color: #6a737d;
      }
      .markdown-body .pl-c1,
      .markdown-body .pl-s .pl-v {
        color: #005cc5;
      }
      .markdown-body .pl-e,
      .markdown-body .pl-en {
        color: #6f42c1;
      }
      .markdown-body .pl-smi,
      .markdown-body .pl-s .pl-s1 {
        color: #24292e;
      }
      .markdown-body .pl-ent {
        color: #22863a;
      }
      .markdown-body .pl-k {
        color: #d73a49;
      }
      .markdown-body .pl-s,
      .markdown-body .pl-pds,
      .markdown-body .pl-s .pl-pse .pl-s1,
      .markdown-body .pl-sr,
      .markdown-body .pl-sr .pl-cce,
      .markdown-body .pl-sr .pl-sre,
      .markdown-body .pl-sr .pl-sra {
        color: #032f62;
      }
      .markdown-body .pl-v,
      .markdown-body .pl-smw {
        color: #e36209;
      }
      .markdown-body .pl-bu {
        color: #b31d28;
      }
      .markdown-body .pl-ii {
        color: #fafbfc;
        background-color: #b31d28;
      }
      .markdown-body .pl-c2 {
        color: #fafbfc;
        background-color: #d73a49;
      }
      .markdown-body .pl-c2::before {
        content: "^M";
      }
      .markdown-body .pl-sr .pl-cce {
        font-weight: bold;
        color: #22863a;
      }
      .markdown-body .pl-ml {
        color: #735c0f;
      }
      .markdown-body .pl-mh,
      .markdown-body .pl-mh .pl-en,
      .markdown-body .pl-ms {
        font-weight: bold;
        color: #005cc5;
      }
      .markdown-body .pl-mi {
        font-style: italic;
        color: #24292e;
      }
      .markdown-body .pl-mb {
        font-weight: bold;
        color: #24292e;
      }
      .markdown-body .pl-md {
        color: #b31d28;
        background-color: #ffeef0;
      }
      .markdown-body .pl-mi1 {
        color: #22863a;
        background-color: #f0fff4;
      }
      .markdown-body .pl-mc {
        color: #e36209;
        background-color: #ffebda;
      }
      .markdown-body .pl-mi2 {
        color: #f6f8fa;
        background-color: #005cc5;
      }
      .markdown-body .pl-mdr {
        font-weight: bold;
        color: #6f42c1;
      }
      .markdown-body .pl-ba {
        color: #586069;
      }
      .markdown-body .pl-sg {
        color: #959da5;
      }
      .markdown-body .pl-corl {
        text-decoration: underline;
        color: #032f62;
      }
      .markdown-body .octicon {
        display: inline-block;
        vertical-align: text-top;
        fill: currentColor;
      }
      .markdown-body a {
        background-color: transparent;
      }
      .markdown-body a:active,
      .markdown-body a:hover {
        outline-width: 0;
      }
      .markdown-body strong {
        font-weight: inherit;
      }
      .markdown-body strong {
        font-weight: bolder;
      }
      .markdown-body h1 {
        font-size: 2em;
        margin: 0.67em 0;
      }
      .markdown-body img {
        border-style: none;
      }
      .markdown-body code,
      .markdown-body kbd,
      .markdown-body pre {
        font-family: monospace, monospace;
        font-size: 1em;
      }
      .markdown-body hr {
        box-sizing: content-box;
        height: 0;
        overflow: visible;
      }
      .markdown-body input {
        font: inherit;
        margin: 0;
      }
      .markdown-body input {
        overflow: visible;
      }
      .markdown-body [type="checkbox"] {
        box-sizing: border-box;
        padding: 0;
      }
      .markdown-body * {
        box-sizing: border-box;
      }
      .markdown-body input {
        font-family: inherit;
        font-size: inherit;
        line-height: inherit;
      }
      .markdown-body a {
        color: #0366d6;
        text-decoration: none;
      }
      .markdown-body a:hover {
        text-decoration: underline;
      }
      .markdown-body strong {
        font-weight: 600;
      }
      .markdown-body hr {
        height: 0;
        margin: 15px 0;
        overflow: hidden;
        background: transparent;
        border: 0;
        border-bottom: 1px solid #dfe2e5;
      }
      .markdown-body hr::before {
        display: table;
        content: "";
      }
      .markdown-body hr::after {
        display: table;
        clear: both;
        content: "";
      }
      .markdown-body table {
        border-spacing: 0;
        border-collapse: collapse;
      }
      .markdown-body td,
      .markdown-body th {
        padding: 0;
      }
      .markdown-body h1,
      .markdown-body h2,
      .markdown-body h3,
      .markdown-body h4,
      .markdown-body h5,
      .markdown-body h6 {
        margin-top: 0;
        margin-bottom: 0;
      }
      .markdown-body h1 {
        font-size: 32px;
        font-weight: 600;
      }
      .markdown-body h2 {
        font-size: 24px;
        font-weight: 600;
      }
      .markdown-body h3 {
        font-size: 20px;
        font-weight: 600;
      }
      .markdown-body h4 {
        font-size: 16px;
        font-weight: 600;
      }
      .markdown-body h5 {
        font-size: 14px;
        font-weight: 600;
      }
      .markdown-body h6 {
        font-size: 12px;
        font-weight: 600;
      }
      .markdown-body p {
        margin-top: 0;
        margin-bottom: 10px;
      }
      .markdown-body blockquote {
        margin: 0;
      }
      .markdown-body ul,
      .markdown-body ol {
        padding-left: 0;
        margin-top: 0;
        margin-bottom: 0;
      }
      .markdown-body ol ol,
      .markdown-body ul ol {
        list-style-type: lower-roman;
      }
      .markdown-body ul ul ol,
      .markdown-body ul ol ol,
      .markdown-body ol ul ol,
      .markdown-body ol ol ol {
        list-style-type: lower-alpha;
      }
      .markdown-body dd {
        margin-left: 0;
      }
      .markdown-body code {
        font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
        font-size: 12px;
      }
      .markdown-body pre {
        margin-top: 0;
        margin-bottom: 0;
        font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
        font-size: 12px;
      }
      .markdown-body .octicon {
        vertical-align: text-bottom;
      }
      .markdown-body .pl-0 {
        padding-left: 0 !important;
      }
      .markdown-body .pl-1 {
        padding-left: 4px !important;
      }
      .markdown-body .pl-2 {
        padding-left: 8px !important;
      }
      .markdown-body .pl-3 {
        padding-left: 16px !important;
      }
      .markdown-body .pl-4 {
        padding-left: 24px !important;
      }
      .markdown-body .pl-5 {
        padding-left: 32px !important;
      }
      .markdown-body .pl-6 {
        padding-left: 40px !important;
      }
      .markdown-body::before {
        display: table;
        content: "";
      }
      .markdown-body::after {
        display: table;
        clear: both;
        content: "";
      }
      .markdown-body>*:first-child {
        margin-top: 0 !important;
      }
      .markdown-body>*:last-child {
        margin-bottom: 0 !important;
      }
      .markdown-body a:not([href]) {
        color: inherit;
        text-decoration: none;
      }
      .markdown-body .anchor {
        float: left;
        padding-right: 4px;
        margin-left: -20px;
        line-height: 1;
      }
      .markdown-body .anchor:focus {
        outline: none;
      }
      .markdown-body p,
      .markdown-body blockquote,
      .markdown-body ul,
      .markdown-body ol,
      .markdown-body dl,
      .markdown-body table,
      .markdown-body pre {
        margin-top: 0;
        margin-bottom: 16px;
      }
      .markdown-body hr {
        height: 0.25em;
        padding: 0;
        margin: 24px 0;
        background-color: #e1e4e8;
        border: 0;
      }
      .markdown-body blockquote {
        padding: 0 1em;
        color: #6a737d;
        border-left: 0.25em solid #dfe2e5;
      }
      .markdown-body blockquote>:first-child {
        margin-top: 0;
      }
      .markdown-body blockquote>:last-child {
        margin-bottom: 0;
      }
      .markdown-body kbd {
        display: inline-block;
        padding: 3px 5px;
        font-size: 11px;
        line-height: 10px;
        color: #444d56;
        vertical-align: middle;
        background-color: #fafbfc;
        border: solid 1px #c6cbd1;
        border-bottom-color: #959da5;
        border-radius: 3px;
        box-shadow: inset 0 -1px 0 #959da5;
      }
      .markdown-body h1,
      .markdown-body h2,
      .markdown-body h3,
      .markdown-body h4,
      .markdown-body h5,
      .markdown-body h6 {
        margin-top: 24px;
        margin-bottom: 16px;
        font-weight: 600;
        line-height: 1.25;
      }
      .markdown-body h1 .octicon-link,
      .markdown-body h2 .octicon-link,
      .markdown-body h3 .octicon-link,
      .markdown-body h4 .octicon-link,
      .markdown-body h5 .octicon-link,
      .markdown-body h6 .octicon-link {
        color: #1b1f23;
        vertical-align: middle;
        visibility: hidden;
      }
      .markdown-body h1:hover .anchor,
      .markdown-body h2:hover .anchor,
      .markdown-body h3:hover .anchor,
      .markdown-body h4:hover .anchor,
      .markdown-body h5:hover .anchor,
      .markdown-body h6:hover .anchor {
        text-decoration: none;
      }
      .markdown-body h1:hover .anchor .octicon-link,
      .markdown-body h2:hover .anchor .octicon-link,
      .markdown-body h3:hover .anchor .octicon-link,
      .markdown-body h4:hover .anchor .octicon-link,
      .markdown-body h5:hover .anchor .octicon-link,
      .markdown-body h6:hover .anchor .octicon-link {
        visibility: visible;
      }
      .markdown-body h1 {
        padding-bottom: 0.3em;
        font-size: 2em;
        border-bottom: 1px solid #eaecef;
      }
      .markdown-body h2 {
        padding-bottom: 0.3em;
        font-size: 1.5em;
        border-bottom: 1px solid #eaecef;
      }
      .markdown-body h3 {
        font-size: 1.25em;
      }
      .markdown-body h4 {
        font-size: 1em;
      }
      .markdown-body h5 {
        font-size: 0.875em;
      }
      .markdown-body h6 {
        font-size: 0.85em;
        color: #6a737d;
      }
      .markdown-body ul,
      .markdown-body ol {
        padding-left: 2em;
      }
      .markdown-body ul ul,
      .markdown-body ul ol,
      .markdown-body ol ol,
      .markdown-body ol ul {
        margin-top: 0;
        margin-bottom: 0;
      }
      .markdown-body li {
        word-wrap: break-all;
      }
      .markdown-body li>p {
        margin-top: 16px;
      }
      .markdown-body li+li {
        margin-top: 0.25em;
      }
      .markdown-body dl {
        padding: 0;
      }
      .markdown-body dl dt {
        padding: 0;
        margin-top: 16px;
        font-size: 1em;
        font-style: italic;
        font-weight: 600;
      }
      .markdown-body dl dd {
        padding: 0 16px;
        margin-bottom: 16px;
      }
      .markdown-body table {
        display: block;
        width: 100%;
        overflow: auto;
      }
      .markdown-body table th {
        font-weight: 600;
      }
      .markdown-body table th,
      .markdown-body table td {
        padding: 6px 13px;
        border: 1px solid #dfe2e5;
      }
      .markdown-body table tr {
        background-color: #fff;
        border-top: 1px solid #c6cbd1;
      }
      .markdown-body table tr:nth-child(2n) {
        background-color: #f6f8fa;
      }
      .markdown-body img {
        max-width: 100%;
        box-sizing: content-box;
        background-color: #fff;
      }
      .markdown-body img[align=right] {
        padding-left: 20px;
      }
      .markdown-body img[align=left] {
        padding-right: 20px;
      }
      .markdown-body code {
        padding: 0.2em 0.4em;
        margin: 0;
        font-size: 85%;
        background-color: rgba(27,31,35,0.05);
        border-radius: 3px;
      }
      .markdown-body pre {
        word-wrap: normal;
      }
      .markdown-body pre>code {
        padding: 0;
        margin: 0;
        font-size: 100%;
        word-break: normal;
        white-space: pre;
        background: transparent;
        border: 0;
      }
      .markdown-body .highlight {
        margin-bottom: 16px;
      }
      .markdown-body .highlight pre {
        margin-bottom: 0;
        word-break: normal;
      }
      .markdown-body .highlight pre,
      .markdown-body pre {
        padding: 16px;
        overflow: auto;
        font-size: 85%;
        line-height: 1.45;
        background-color: #f6f8fa;
        border-radius: 3px;
      }
      .markdown-body pre code {
        display: inline;
        max-width: auto;
        padding: 0;
        margin: 0;
        overflow: visible;
        line-height: inherit;
        word-wrap: normal;
        background-color: transparent;
        border: 0;
      }
      .markdown-body .full-commit .btn-outline:not(:disabled):hover {
        color: #005cc5;
        border-color: #005cc5;
      }
      .markdown-body kbd {
        display: inline-block;
        padding: 3px 5px;
        font: 11px "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
        line-height: 10px;
        color: #444d56;
        vertical-align: middle;
        background-color: #fafbfc;
        border: solid 1px #d1d5da;
        border-bottom-color: #c6cbd1;
        border-radius: 3px;
        box-shadow: inset 0 -1px 0 #c6cbd1;
      }
      .markdown-body :checked+.radio-label {
        position: relative;
        z-index: 1;
        border-color: #0366d6;
      }
      .markdown-body .task-list-item {
        list-style-type: none;
      }
      .markdown-body .task-list-item+.task-list-item {
        margin-top: 3px;
      }
      .markdown-body .task-list-item input {
        margin: 0 0.2em 0.25em -1.6em;
        vertical-align: middle;
      }
      .markdown-body hr {
        border-bottom-color: #eee;
      }
      /* github-markdown.css overrides */
      .markdown-body h2 {
        margin-bottom: 0;
        margin-top: 0;
      }

      .hll { background-color: #ffffcc }
      .c { color: #008000 } /* Comment */
      .err { border: 1px solid #FF0000 } /* Error */
      .k { color: #0000ff } /* Keyword */
      .cm { color: #008000 } /* Comment.Multiline */
      .cp { color: #0000ff } /* Comment.Preproc */
      .c1 { color: #008000 } /* Comment.Single */
      .cs { color: #008000 } /* Comment.Special */
      .ge { font-style: italic } /* Generic.Emph */
      .gh { font-weight: bold } /* Generic.Heading */
      .gp { font-weight: bold } /* Generic.Prompt */
      .gs { font-weight: bold } /* Generic.Strong */
      .gu { font-weight: bold } /* Generic.Subheading */
      .kc { color: #0000ff } /* Keyword.Constant */
      .kd { color: #0000ff } /* Keyword.Declaration */
      .kn { color: #0000ff } /* Keyword.Namespace */
      .kp { color: #0000ff } /* Keyword.Pseudo */
      .kr { color: #0000ff } /* Keyword.Reserved */
      .kt { color: #2b91af } /* Keyword.Type */
      .s { color: #a31515 } /* Literal.String */
      .nc { color: #2b91af } /* Name.Class */
      .ow { color: #0000ff } /* Operator.Word */
      .sb { color: #a31515 } /* Literal.String.Backtick */
      .sc { color: #a31515 } /* Literal.String.Char */
      .sd { color: #a31515 } /* Literal.String.Doc */
      .s2 { color: #a31515 } /* Literal.String.Double */
      .se { color: #a31515 } /* Literal.String.Escape */
      .sh { color: #a31515 } /* Literal.String.Heredoc */
      .si { color: #a31515 } /* Literal.String.Interpol */
      .sx { color: #a31515 } /* Literal.String.Other */
      .sr { color: #a31515 } /* Literal.String.Regex */
      .s1 { color: #a31515 } /* Literal.String.Single */
      .ss { color: #a31515 } /* Literal.String.Symbol */

    </style>

    <script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Product",
  "name": "leemengtaiwan/deep-learning-resources",
  "description": "由淺入深的深度學習資源 Collection of deep learning materials for everyone",
  "url": "https://awesomeopensource.com/project/leemengtaiwan/deep-learning-resources",
  "sku": "project/leemengtaiwan/deep-learning-resources",
  "mpn": "project/leemengtaiwan/deep-learning-resources",
  "image": "https://github.com/leemengtaiwan.png",
  "brand": {
    "@type": "Brand",
    "name": "leemengtaiwan/deep-learning-resources",
    "logo": "https://github.com/leemengtaiwan.png"
  },
  "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "4.57",
    "ratingCount": "401"
  }
}
</script>


    <meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="Mh/wGTXS1dZI2dQnJszE02shGzaaqyt/k0DmsKxgQTYBP05MMdLD2QKZu9cRj/tmCVDzMz+uVbIInSB5Y8VM8w==" />
  <script async src='/cdn-cgi/bm/cv/669835187/api.js'></script></head>

  <body>
    <script type="application/json" id="js-react-on-rails-context">{"railsEnv":"production","inMailer":false,"i18nLocale":"en","i18nDefaultLocale":"en","rorVersion":"11.2.1","rorPro":false,"href":"http://awesomeopensource.com/project/leemengtaiwan/deep-learning-resources","location":"/project/leemengtaiwan/deep-learning-resources","scheme":"http","host":"awesomeopensource.com","port":null,"pathname":"/project/leemengtaiwan/deep-learning-resources","search":null,"httpAcceptLanguage":null,"serverSide":false}</script>
<div id="Root-react-component-ceaf6eed-261c-49b7-948b-803511ff3bf3"><div style="font-family:helvetica, arial, sans-serif" data-reactroot=""><div class="aos_fixed_header_container"><div class="display_flex margin_bottom_15"><div style="margin-right:1%;display:flex;align-items:center"><a href="https://awesomeopensource.com" title="Awesome Open Source Home" rel="canonical"><img src="https://awesomeopensource.com/awesome.gif" class="aos_logo_image"/></a></div><div class="aos_header_title_container_larger_than_ipad"><div style="white-space:nowrap;font-size:24px;font-weight:bold;display:flex;flex-direction:row;align-items:center;height:100%"><div>Awesome Open Source</div></div></div><div class="aos_header_title_container_ipad_and_smaller"><div>Awesome Open Source</div></div><div style="display:flex;flex-wrap:wrap"></div></div><div></div></div><div class="aos_project_screen_container"><div class="aos_project_inner_screen_container"><div class="aos_project_metadata"><div class="aos_project_metadata_title">Deep <!-- -->Learning <!-- -->Resources</div><div class="aos_project_metadata_description">由淺入深的深度學習資源 Collection of deep learning materials for everyone</div><div class="aos_project_metadata_header">Stars</div><span class="aos_project_metadata_content">401</span><div class="project_page_ad_slot_1"><ins class="adsbygoogle" style="display:block" data-ad-format="fluid" data-ad-layout-key="-gv-v+1t-u+27" data-ad-client="ca-pub-1878397330895234" data-ad-slot="5649017589"></ins></div><div class="aos_project_metadata_header aos_license_text_metadata">License</div><span class="aos_project_metadata_content">mit</span><span><div class="aos_project_metadata_header"><a href="https://leemeng.tw/deep-learning-resources.html" target="_blank" class="aos_grey_link" rel="nofollow">Homepage</a></div> <span class="aos_project_metadata_content"><a href="https://leemeng.tw/deep-learning-resources.html" target="_blank">https://leemeng.tw/deep-learning-resources.html</a></span></span><div class="aos_project_metadata_header"><a href="https://github.com/leemengtaiwan/deep-learning-resources" target="_blank" class="aos_grey_link" rel="nofollow">Repository</a></div><span class="aos_project_metadata_content"><a href="https://github.com/leemengtaiwan/deep-learning-resources" target="_blank">https://github.com/leemengtaiwan/deep-learning-resources</a></span><div class="aos_project_metadata_header"><a href="https://github.com/leemengtaiwan/deep-learning-resources/issues" target="_blank" class="aos_grey_link" rel="nofollow">Open Issues</a></div><span class="aos_project_metadata_content"><a href="https://github.com/leemengtaiwan/deep-learning-resources/issues" target="_blank">0</a></span><div class="aos_project_metadata_header">Most Recent Commit</div><span class="aos_project_metadata_content">14 days ago</span><div class="aos_project_metadata_header">Related Projects</div><span class="aos_project_metadata_content"><a href="https://awesomeopensource.com/projects/python"><div style="display:block;text-decoration:none;padding-bottom:10px">python<span> (<!-- -->49,729<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/jupyter-notebook"><div style="display:block;text-decoration:none;padding-bottom:10px">jupyter-notebook<span> (<!-- -->5,677<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/deep-learning"><div style="display:block;text-decoration:none;padding-bottom:10px">deep-learning<span> (<!-- -->3,659<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/pytorch"><div style="display:block;text-decoration:none;padding-bottom:10px">pytorch<span> (<!-- -->2,132<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/tensorflow"><div style="display:block;text-decoration:none;padding-bottom:10px">tensorflow<span> (<!-- -->2,063<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/neural-networks"><div style="display:block;text-decoration:none;padding-bottom:10px">neural-networks<span> (<!-- -->401<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/tools"><div style="display:block;text-decoration:none;padding-bottom:10px">tools<span> (<!-- -->318<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/gan"><div style="display:block;text-decoration:none;padding-bottom:10px">gan<span> (<!-- -->314<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/deeplearning"><div style="display:block;text-decoration:none;padding-bottom:10px">deeplearning<span> (<!-- -->270<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/generative-adversarial-network"><div style="display:block;text-decoration:none;padding-bottom:10px">generative-adversarial-network<span> (<!-- -->234<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/chinese"><div style="display:block;text-decoration:none;padding-bottom:10px">chinese<span> (<!-- -->187<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/collections"><div style="display:block;text-decoration:none;padding-bottom:10px">collections<span> (<!-- -->60<!-- -->)</span></div></a><a href="https://awesomeopensource.com/projects/courses"><div style="display:block;text-decoration:none;padding-bottom:10px">courses<span> (<!-- -->33<!-- -->)</span></div></a></span></div><div class="markdown-body"><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<div align="center">
  <a href="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/general/paper-ball.jpg" target="_blank" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/general/paper-ball.jpg" style="max-width:100%;"></a>
</div>
<hr>
<p>這裡紀錄了我在學習<a href="https://leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html" rel="nofollow">深度學習</a>時蒐集的一些線上資源。內容由淺入深，而且會不斷更新，希望能幫助你順利地開始學習：）</p>
<h2>
<a id="本文章節" class="anchor" href="#%E6%9C%AC%E6%96%87%E7%AB%A0%E7%AF%80" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>本文章節</h2>
<ul>
<li><a href="#playground">遊玩空間</a></li>
<li><a href="#courses">線上課程</a></li>
<li><a href="#tools">實用工具</a></li>
<li><a href="#tutorials">其他教材</a></li>
<li><a href="#blogs">優質文章</a></li>
<li><a href="#papers">經典論文</a></li>
<li><a href="#collections">其他整理</a></li>
</ul>
<h2>
<a id="遊玩空間" class="anchor" href="#%E9%81%8A%E7%8E%A9%E7%A9%BA%E9%96%93" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><div>遊玩空間</div>
</h2>
<p>這節列舉了一些透過瀏覽器就能馬上開始遊玩 / 體驗深度學習的應用。作為這些應用的使用者，你可以先高層次、直觀地了解深度學習能做些什麼。之後有興趣再進一步了解背後原理。</p>
<p>這小節最適合：</p>
<ul>
<li>想要快速體會深度學習如何被應用在真實世界的好奇寶寶</li>
<li>想要直觀理解<a href="https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">類神經網路（Artifical Neural Network）</a>運作方式的人</li>
<li>想從別人的深度學習應用取得一些靈感的開發者</li>
</ul>
<table>
<thead>
<tr>
<th align="center"><a href="https://playground.tensorflow.org/" rel="nofollow">Deep Playground</a></th>
<th align="center"><a href="https://cs.stanford.edu/people/karpathy/convnetjs/index.html" rel="nofollow">ConvNetJS</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://playground.tensorflow.org/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/deep-playground.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://cs.stanford.edu/people/karpathy/convnetjs/index.html" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/convnetjs.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="deep-playground" class="anchor" href="#deep-playground" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://playground.tensorflow.org/" rel="nofollow">Deep Playground</a>
</h3>
<ul>
<li>由 <a href="https://github.com/tensorflow/playground" rel="nofollow">Tensorflow 團隊</a>推出，模擬訓練一個類神經網路的過程並了解其運作原理</li>
<li>可以搭配這篇 <a href="https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/playground-exercises" rel="nofollow">Introduction to Neural Networks: Playground Exercises</a> 學習</li>
</ul>
<h3>
<a id="convnetjs" class="anchor" href="#convnetjs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://cs.stanford.edu/people/karpathy/convnetjs/" rel="nofollow">ConvNetJS</a>
</h3>
<ul>
<li>訓練類神經網路來解決經典的 <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html" rel="nofollow">MNIST 手寫數字辨識問題</a>、<a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html" rel="nofollow">圖片生成</a>以及<a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html" rel="nofollow">增強式學習</a>
</li>
<li>由 Tesla 的 AI 負責人 <a href="https://cs.stanford.edu/people/karpathy/" rel="nofollow">Andrej Karpathy</a> 建立</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://magenta.tensorflow.org/" rel="nofollow">Magenta</a></th>
<th align="center"><a href="https://experiments.withgoogle.com/collection/ai" rel="nofollow">Google AI Experiments</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://magenta.tensorflow.org/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/magenta.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://experiments.withgoogle.com/collection/ai" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/google-ai-experiment.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="magenta" class="anchor" href="#magenta" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://magenta.tensorflow.org/" rel="nofollow">Magenta</a>
</h3>
<ul>
<li>一個利用<a href="https://zh.wikipedia.org/zh-hant/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" rel="nofollow">機器學習</a>來協助人們進行音樂以及藝術創作的開源專案</li>
<li>可以在網站上的 <a href="https://magenta.tensorflow.org/demos" rel="nofollow">Demo 頁面</a>嘗試各種由深度學習驅動的音樂 / 繪畫應用（如彈奏鋼琴、擊鼓）</li>
</ul>
<h3>
<a id="google-ai-experiments" class="anchor" href="#google-ai-experiments" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://experiments.withgoogle.com/collection/ai" rel="nofollow">Google AI Experiments</a>
</h3>
<ul>
<li>這邊展示了接近 40 個利用圖片、語言以及音樂來與使用者產生互動的機器學習 Apps，值得慢慢探索</li>
<li>知名例子有 <a href="https://quickdraw.withgoogle.com/" rel="nofollow">Quick Draw</a> 以及 <a href="https://teachablemachine.withgoogle.com/" rel="nofollow">Teachable Machine</a>，將在下方介紹</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://quickdraw.withgoogle.com/" rel="nofollow">Quick Draw</a></th>
<th align="center"><a href="https://teachablemachine.withgoogle.com/" rel="nofollow">Teachable Machine</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://quickdraw.withgoogle.com/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/quickdraw.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://teachablemachine.withgoogle.com/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/teachable-machine.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="quick-draw" class="anchor" href="#quick-draw" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://quickdraw.withgoogle.com/" rel="nofollow">Quick Draw</a>
</h3>
<ul>
<li>由 Google 推出的知名手寫塗鴉辨識，使用的神經網路架構有常見的<a href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" rel="nofollow">卷積神經網路 CNN </a>以及<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF_1" rel="nofollow">循環神經網路 RNN</a>
</li>
<li>該深度學習模型會不斷將最新的筆觸當作輸入來預測使用者想畫的物件。你會驚嘆於她精準且即時的判斷</li>
</ul>
<h3>
<a id="teachable-machine" class="anchor" href="#teachable-machine" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://teachablemachine.withgoogle.com/" rel="nofollow">Teachable Machine</a>
</h3>
<ul>
<li>利用電腦 / 手機上的相機來訓練能將影像對應到其他圖片、音訊的神經網路，饒富趣味</li>
<li>透過這例子，你將暸解機器學習的神奇之處以及其侷限所在</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://tenso.rs/demos/fast-neural-style/" rel="nofollow">Fast Neural Style</a></th>
<th align="center"><a href="https://js.tensorflow.org/" rel="nofollow">TensorFlow.js</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://tenso.rs/demos/fast-neural-style/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/fast-neural-style.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://js.tensorflow.org/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/human-pose-estimation.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="fast-neural-style" class="anchor" href="#fast-neural-style" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://tenso.rs/demos/fast-neural-style/" rel="nofollow">Fast Neural Style</a>
</h3>
<ul>
<li>展示如何使用 WebGL 在瀏覽器快速地進行<a href="https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398" rel="nofollow">神經風格轉換 Neural Style Transfer</a>
</li>
<li>你可以選擇任何一張圖片，並在此網站上將其畫風轉變成指定的藝術照</li>
<li>
<a href="https://deepart.io/" rel="nofollow">Deepart.io</a> 也提供類似服務</li>
</ul>
<h3>
<a id="tensorflowjs" class="anchor" href="#tensorflowjs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://js.tensorflow.org/" rel="nofollow">TensorFlow.js</a>
</h3>
<ul>
<li>TensorFlow.js 頁面有多個利用 JavaScript 實現的深度學習應用，如上圖中的<a href="https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5" rel="nofollow">人類姿勢估計 Human Pose Estimation</a>。</li>
<li>你可以在該應用裡頭打開自己的攝影機，看該應用能不能偵測到你與朋友的姿勢。</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://poloclub.github.io/ganlab/" rel="nofollow">GAN Lab</a></th>
<th align="center"><a href="https://talktotransformer.com/" rel="nofollow">Talk to Transformer</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://poloclub.github.io/ganlab/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/gan-lab.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://talktotransformer.com/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/talk_to_transformer.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="gan-lab" class="anchor" href="#gan-lab" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://poloclub.github.io/ganlab/" rel="nofollow">GAN Lab</a>
</h3>
<ul>
<li>
<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C" rel="nofollow">對抗生成網路（<strong>G</strong>enerative <strong>A</strong>dversarial <strong>N</strong>etwork，簡稱GAN）</a>是非監督式學習的一種方法，通過讓兩個神經網路相互博弈的方式進行學習。此網站以 <a href="https://js.tensorflow.org/" rel="nofollow">TensorFlow.js</a> 實作 GAN 中兩個神經網路的學習過程，幫助有興趣的你更直觀地理解神奇的 GAN 的運作方式</li>
</ul>
<h3>
<a id="talk-to-transformer" class="anchor" href="#talk-to-transformer" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://talktotransformer.com/" rel="nofollow">Talk to Transformer</a>
</h3>
<ul>
<li>展示了一個由 OpenAI 推出，名為 <a href="https://openai.com/blog/better-language-models/" rel="nofollow">GPT-2 的無監督式語言模型</a>。該模型以 Google 發表的神經網路架構 <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="nofollow">Transformer</a> 為基底，在給定一段魔戒或是復仇者聯盟的文字內容，該模型可以自己生成唯妙唯俏的延伸劇情。你也可以嘗試 <a href="https://gpt2.apps.allenai.org/?text=Joel%20is" rel="nofollow">AllenAI GPT-2 Explorer</a> 來觀察 GPT-2 預測下個字的機率。</li>
<li>想要深入了解 Transformer 或 GPT-2，推薦閱讀：
<ul>
<li><a href="https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html" rel="nofollow">淺談神經機器翻譯 &amp; 用 Transformer 與 TensorFlow 2 英翻中</a></li>
<li><a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html" rel="nofollow">直觀理解 GPT-2 語言模型並生成金庸武俠小說</a></li>
<li><a href="https://jalammar.github.io/illustrated-gpt2/" rel="nofollow">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a></li>
</ul>
</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://www.nvidia.com/en-us/research/ai-playground/" rel="nofollow">NVIDIA AI PLAYGROUND</a></th>
<th align="center"><a href="https://grover.allenai.org/" rel="nofollow">Grover</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://www.nvidia.com/en-us/research/ai-playground/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/nvidia-ai-playground.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://grover.allenai.org/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/grover.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="nvidia-ai-playground" class="anchor" href="#nvidia-ai-playground" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://www.nvidia.com/en-us/research/ai-playground/" rel="nofollow">NVIDIA AI PLAYGROUND</a>
</h3>
<ul>
<li>提供 <a href="https://arxiv.org/abs/1903.07291" rel="nofollow">GauGAN</a> 的線上展示，讓你可以利用簡單的筆觸來生成真實世界的風景圖片，也能上傳自己的圖片做風格轉換</li>
<li>提供 <a href="https://arxiv.org/abs/1804.07723" rel="nofollow">Image Impainting</a> 服務，讓使用者自由抹去部分圖片並讓 AI 自動生成被抹去的區塊</li>
</ul>
<h3>
<a id="grover" class="anchor" href="#grover" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://grover.allenai.org/" rel="nofollow">Grover</a>
</h3>
<ul>
<li>一個偵測 / 生成神經假新聞（Neural Fake News）的研究，其網頁展示如何自動生成假新聞。</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://waifulabs.com" rel="nofollow">Waifu Vending Machine</a></th>
<th align="center"><a href="https://www.thiswaifudoesnotexist.net/" rel="nofollow">This Waifu Does Not Exist</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://waifulabs.com" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/waifulabs.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://www.thiswaifudoesnotexist.net/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/thiswaifudoesnotexist.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="waifu-vending-machine" class="anchor" href="#waifu-vending-machine" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://waifulabs.com" rel="nofollow">Waifu Vending Machine</a>
</h3>
<ul>
<li>Waifu 來自日文 ワイフ，指的是一些非常受到歡迎、且被不少玩家/觀眾視為妻子的動漫女性角色。<a href="https://twitter.com/SizigiStudios" rel="nofollow">Sizigi Studios</a> 團隊利用 GAN 隨機初始 16 名虛擬動漫角色，讓使用者可以進一步依照喜愛來創造專屬於自己的 Waifu。</li>
<li>Waifu Vending Machine 產生的 Waifu 品質很高，使用者可以下載並分享自己創造的 Waifu，也可以選擇購買印製該 Waifu 的海報與抱枕。</li>
</ul>
<h3>
<a id="this-waifu-does-not-exist" class="anchor" href="#this-waifu-does-not-exist" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://www.thiswaifudoesnotexist.net/" rel="nofollow">This Waifu Does Not Exist</a>
</h3>
<ul>
<li>以 Nvidia 的 <a href="https://github.com/NVlabs/stylegan" rel="nofollow">StyleGAN</a> 隨機生成的 Waifu（右圖左側）。作者 <a href="https://www.gwern.net/" rel="nofollow">Gwern</a> 同時也使用<a href="https://blog.openai.com/better-language-models/" rel="nofollow">開源的小型 GPT-2</a> 隨機生成一段動漫劇情（右圖右側）。自釋出後已超越一百萬使用者拜訪該網站。</li>
<li>你也可以用大螢幕查看作者的另個相關網站：<a href="https://www.obormot.net/demos/these-waifus-do-not-exist" rel="nofollow">These Waifus Do Not Exist</a>，用全畫面一次「觀賞」數十名隨機生成的 Waifus。</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="http://www.deeplearning.ai/ai-notes/" rel="nofollow">AI Notes</a></th>
<th align="center"><a href="https://anomagram.fastforwardlabs.com/#/" rel="nofollow">Anomagram</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="http://www.deeplearning.ai/ai-notes/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/deeplearning-ai-notes.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://anomagram.fastforwardlabs.com/#/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/anomagram.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="ai-notes" class="anchor" href="#ai-notes" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="http://www.deeplearning.ai/ai-notes/" rel="nofollow">AI Notes</a>
</h3>
<ul>
<li>AI Notes 是 <a href="#deep-learning-specialization--coursera">吳恩達的 Deep Learning 專項課程</a>的輔助教材，使用數學證明以及由 TensorFlow.js 建立的線上 demo 讓你可以直觀地學習<a href="http://www.deeplearning.ai/ai-notes/initialization/" rel="nofollow">如何初始化神經網路權重</a>及<a href="http://www.deeplearning.ai/ai-notes/optimization/" rel="nofollow">如何最佳化模型權重</a>
</li>
<li>縮圖為 <a href="http://www.deeplearning.ai/ai-notes/optimization/" rel="nofollow">Parameter optimization in neural networks</a> 單元中使用不同 Optimiziers 訓練模型的線上 demo</li>
</ul>
<h3>
<a id="anomagram" class="anchor" href="#anomagram" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://anomagram.fastforwardlabs.com/#/" rel="nofollow">Anomagram</a>
</h3>
<ul>
<li>Anomagram 是一個以 Tensorflow.js 實作，可以建立、訓練並測試能夠用來做異常檢測的 Autoencoder。</li>
</ul>
<h2>
<a id="線上課程" class="anchor" href="#%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><div>線上課程</div>
</h2>
<p>看完<a href="#playground">遊玩空間</a>的大量實際應用，相信你已經迫不及待地想要開始學習強大的深度學習技術了。</p>
<p>這節列舉了一些有用的線上課程以及學習教材，幫助你掌握深度學習的基本知識（沒有特別註明的話皆為免費存取）。</p>
<p>另外值得一提的是，大部分課程都要求一定程度的 <a href="https://www.python.org/" rel="nofollow">Python</a> 程式能力。</p>
<table>
<thead>
<tr>
<th align="center"><a href="http://speech.ee.ntu.edu.tw/%7Etlkagk/courses_ML19.html" rel="nofollow">李宏毅教授的機器學習 / 深度學習課程</a></th>
<th align="center"><a href="https://www.coursera.org/specializations/deep-learning" rel="nofollow">Deep Learning Specialization @ Coursera</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/Hung-Yi-Lee-ml-courses.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://www.coursera.org/specializations/deep-learning" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/deep-learning-specification-coursera.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="李宏毅教授的機器學習--深度學習課程" class="anchor" href="#%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%95%99%E6%8E%88%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92--%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E8%AA%B2%E7%A8%8B" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="http://speech.ee.ntu.edu.tw/%7Etlkagk/courses_ML20.html" rel="nofollow">李宏毅教授的機器學習 / 深度學習課程</a>
</h3>
<ul>
<li>大概是全世界最好、最完整的深度學習<b>中文</b>學習資源，且作業皆提供 Colab 筆記本範例。</li>
<li>影片內容涵蓋基本理論（約 10 小時觀看時間）一直到進階的<a href="https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C" rel="nofollow">生成對抗網路 GAN</a> 以及<a href="https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" rel="nofollow">強化學習 RL</a>。</li>
<li>想學語音辨識或是自然語言處理則可參考教授的<a href="http://speech.ee.ntu.edu.tw/%7Etlkagk/courses_DLHLP20.html" rel="nofollow">用深度學習處理人類語言</a>。</li>
<li>
<a href="https://github.com/datawhalechina/leeml-notes" rel="nofollow">李宏毅机器学习笔记(LeeML-Notes，簡體)</a> 則將教授上課的影片內容轉為筆記，方便瀏覽課程內容。</li>
</ul>
<h3>
<a id="deep-learning-specialization--coursera" class="anchor" href="#deep-learning-specialization--coursera" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://www.coursera.org/specializations/deep-learning" rel="nofollow">Deep Learning Specialization @ Coursera</a>
</h3>
<ul>
<li>原 Google Brain 的<a href="https://zh.wikipedia.org/wiki/%E5%90%B4%E6%81%A9%E8%BE%BE" rel="nofollow">吳恩達</a>教授開授的整個深度學習專項課程共分五堂課，從<a href="https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning" rel="nofollow">神經網路的基礎</a>到能夠進行機器翻譯、語音辨識的<a href="https://www.coursera.org/learn/nlp-sequence-models" rel="nofollow">序列模型</a>，每堂課預計 1 個月完成，收費採訂閱制</li>
<li>程式作業會交互使用 <a href="http://www.numpy.org/" rel="nofollow">Numpy</a>、<a href="https://keras.io/" rel="nofollow">Keras</a> 以及 <a href="https://www.tensorflow.org/" rel="nofollow">TensorFlow</a> 來實作深度學習模型</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://course.fast.ai/index.html" rel="nofollow">Practical Deep Learning For Coders @ fast.ai</a></th>
<th align="center"><a href="https://www.kaggle.com/learn/deep-learning" rel="nofollow">Deep Learning @ Kaggle Learn</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://course.fast.ai/index.html" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/fast-ai.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://www.kaggle.com/learn/deep-learning" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/kaggle-learn-dl.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="practical-deep-learning-for-coders--fastai" class="anchor" href="#practical-deep-learning-for-coders--fastai" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://course.fast.ai/index.html" rel="nofollow">Practical Deep Learning For Coders @ fast.ai</a>
</h3>
<ul>
<li>7 週課程，一週約需安排 10 小時上課。該課程由<a href="https://www.kaggle.com/jhoward" rel="nofollow">傑里米·霍華德</a>來講解深度學習，其在知名數據建模和數據分析競賽平台 <a href="https://www.kaggle.com/" rel="nofollow">Kaggle</a> 維持兩年的世界第一</li>
</ul>
<h3>
<a id="deep-learning--kaggle-learn" class="anchor" href="#deep-learning--kaggle-learn" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://www.kaggle.com/learn/deep-learning" rel="nofollow">Deep Learning @ Kaggle Learn</a>
</h3>
<ul>
<li>14 堂課程，主要使用 TensorFlow 實作深度學習模型</li>
<li>內容主要專注在<a href="https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89" rel="nofollow">電腦視覺（Computer Vision）</a>以及如何應用<a href="https://en.wikipedia.org/wiki/Transfer_learning" rel="nofollow">遷移學習（Transfer Learning）</a>
</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://www.elementsofai.com/" rel="nofollow">Elements of Artificial Intelligence</a></th>
<th align="center"><a href="https://deeplearning.mit.edu/" rel="nofollow">MIT Deep Learning</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://www.elementsofai.com/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/elementsofai.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://selfdrivingcars.mit.edu/deeptraffic" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/mlt-deep-learning.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="elements-of-artificial-intelligence" class="anchor" href="#elements-of-artificial-intelligence" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://www.elementsofai.com/" rel="nofollow">Elements of Artificial Intelligence</a>
</h3>
<ul>
<li>芬蘭最高學府<a href="https://zh.wikipedia.org/wiki/%E8%B5%AB%E5%B0%94%E8%BE%9B%E5%9F%BA%E5%A4%A7%E5%AD%A6" rel="nofollow">赫爾辛基大學</a>推出的 AI 課程。此課程目的在於讓所有人都能了解 AI，不需要任何程式經驗。這堂課非常適合完全沒有接觸過深度學習或是相關領域的人</li>
<li>課程分 6 個部分，包含「何謂 AI ？」、「真實世界的 AI」、「機器學習」以及「神經網路」等章節</li>
</ul>
<h3>
<a id="mit-deep-learning" class="anchor" href="#mit-deep-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://deeplearning.mit.edu/" rel="nofollow">MIT Deep Learning</a>
</h3>
<ul>
<li>麻省理工學院推出的深度學習課程，內容包含深度學習基礎、深度強化學習以及自動駕駛相關知識。<a href="https://github.com/lexfridman/mit-deep-learning" rel="nofollow">Github Repo</a> 包含了多個教學筆記本，值得參考。</li>
<li>上圖是 <a href="https://selfdrivingcars.mit.edu/deeptraffic/" rel="nofollow">DeepTraffic</a>，由 MIT 的研究科學家 <a href="https://lexfridman.com/" rel="nofollow">Lex Fridman</a> 推出的一個深度強化學習競賽。此競賽目標是建立一個可以在高速公路上駕駛汽車的神經網路。你可以在<a href="https://selfdrivingcars.mit.edu/deeptraffic/" rel="nofollow">這裡</a>看到線上 Demo 以及詳細說明。</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="http://introtodeeplearning.com" rel="nofollow">6.S191: Introduction to Deep Learning</a></th>
<th align="center"><a href="https://www.coursera.org/learn/ai-for-everyone" rel="nofollow">AI For Everyone</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="http://introtodeeplearning.com" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/intro-to-deeplearning-mit.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://www.coursera.org/learn/ai-for-everyone" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/ai-for-everyone.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="mit-6s191-introduction-to-deep-learning" class="anchor" href="#mit-6s191-introduction-to-deep-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="http://introtodeeplearning.com/" rel="nofollow">MIT 6.S191 Introduction to Deep Learning</a>
</h3>
<ul>
<li>麻省理工學院推出的另一堂基礎深度學習課程，介紹深度學習以及其應用。內容涵蓋機器翻譯、圖像辨識以及更多其他應用。此課程使用 Python 以及 TensorFlow 來實作作業，並預期學生具備基礎的微積分（梯度下降、鏈鎖律）以及線性代數（矩陣相乘）。</li>
</ul>
<h3>
<a id="ai-for-everyone" class="anchor" href="#ai-for-everyone" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://www.coursera.org/learn/ai-for-everyone" rel="nofollow">AI For Everyone</a>
</h3>
<ul>
<li>Coursera 課程。<a href="https://zh.wikipedia.org/wiki/%E5%90%B4%E6%81%A9%E8%BE%BE" rel="nofollow">吳恩達</a>教授在這堂簡短的課程裡頭，針對非技術人士以及企業經理人說明何謂 AI、如何建立 AI 專案以及闡述 AI 與社會的關係。此課程十分適合沒有技術背景的讀者。<a href="https://leemeng.tw/10-key-takeaways-from-ai-for-everyone-course.html" rel="nofollow">從 AI For Everyone 學到的 10 個重要 AI 概念</a>則是我個人上完課後整理的心得分享。</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="http://web.stanford.edu/class/cs224n/" rel="nofollow">CS224n: Natural Language Processing with Deep Learning</a></th>
<th align="center"><a href="http://cs231n.stanford.edu/" rel="nofollow">CS231n: Convolutional Neural Networks for Visual Recognition</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://github.com/leemengtaiwan/deep-learning-resources/blob/master/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/cs224n.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://github.com/leemengtaiwan/deep-learning-resources/blob/master/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/cs231n.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="cs224n-natural-language-processing-with-deep-learning" class="anchor" href="#cs224n-natural-language-processing-with-deep-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="http://web.stanford.edu/class/cs224n/" rel="nofollow">CS224n: Natural Language Processing with Deep Learning</a>
</h3>
<ul>
<li>由<a href="http://technews.tw/2018/11/21/stanford-ai-lab-christopher-manning/" rel="nofollow">史丹佛 AI 實驗室的 Christopher Manning 教授</a>從語言學、計算機科學的角度講述自然語言處理的所有必要知識，是想要打好 NLP 基礎的人不可不學的一堂課。課程約有 20 部影片，每部約長 1.5 小時。</li>
</ul>
<h3>
<a id="cs231n-convolutional-neural-networks-for-visual-recognition" class="anchor" href="#cs231n-convolutional-neural-networks-for-visual-recognition" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="http://cs231n.stanford.edu/" rel="nofollow">CS231n: Convolutional Neural Networks for Visual Recognition</a>
</h3>
<ul>
<li>由<a href="http://vision.stanford.edu/index.html" rel="nofollow">史丹佛 Vision Lab 的李飛飛（Fei-Fei Li）教授</a>等人以<a href="http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture02.pdf" rel="nofollow">圖像分類</a>任務為軸心，講述卷積神經網路以及所有電腦視覺的相關基礎知識。這是想要學會使用（卷積）神經網路來處理圖像數據的人不可不學的一堂課。<a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" rel="nofollow">Youtube 上有 16 部 2017 年的課程錄影</a>，每部約長 1 小時。</li>
<li>課程中也包含了不少線上展示，如<a href="http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/" rel="nofollow">線性分類器的 loss 視覺化</a>、<a href="http://vision.stanford.edu/teaching/cs231n-demos/knn/" rel="nofollow">kNN demo</a> 以及圖像分類的 <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" rel="nofollow">CIFAR-10 demo</a>。</li>
</ul>
<h2>
<a id="實用工具" class="anchor" href="#%E5%AF%A6%E7%94%A8%E5%B7%A5%E5%85%B7" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><div>實用工具</div>
</h2>
<p>這節列出一些在你的深度學習路上可以幫得上些忙的工具。</p>
<table>
<thead>
<tr>
<th align="center"><a href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="nofollow">Colaboratory</a></th>
<th align="center"><a href="https://www.tensorflow.org/guide/summaries_and_tensorboard" rel="nofollow">TensorBoard</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/colab.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://www.tensorflow.org/guide/summaries_and_tensorboard" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/tensorboard.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="colaboratory" class="anchor" href="#colaboratory" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="nofollow">Colaboratory</a>
</h3>
<ul>
<li>由 Google 提供的雲端 <a href="https://jupyter.org/" rel="nofollow">Jupyter</a> 筆記本環境，讓你只要用瀏覽器就能馬上開始訓練深度學習模型。你甚至還可以使用一個免費的 <a href="https://www.nvidia.com/en-gb/data-center/tesla-k80/" rel="nofollow">Tesla K80</a> GPU 或 <a href="https://colab.research.google.com/notebooks/tpu.ipynb" rel="nofollow">TPU</a> 來加速訓練自己的模型</li>
<li>該計算環境也能與自己的 <a href="https://colab.research.google.com/notebooks/io.ipynb" rel="nofollow">Google Drive</a> 做連結，讓運算雲端化的同時將筆記本 / 模型結果都同步到自己的筆電上</li>
</ul>
<h3>
<a id="tensorboard" class="anchor" href="#tensorboard" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://www.tensorflow.org/guide/summaries_and_tensorboard" rel="nofollow">TensorBoard</a>
</h3>
<ul>
<li>TensorBoard 是一個視覺化工具，方便我們了解、除錯並最佳化自己訓練的深度學習模型</li>
<li>除了 TensorFlow 以外，其他基於 Python 的機器學習框架大多也可以透過 <a href="https://github.com/lanpa/tensorboardX" rel="nofollow">tensorboardX</a> 來使用 TensorBoard</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://projector.tensorflow.org/" rel="nofollow">Embedding Projector</a></th>
<th align="center"><a href="https://github.com/tensorflow/lucid" rel="nofollow">Lucid</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://projector.tensorflow.org/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/embedding-projector.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://github.com/tensorflow/lucid" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/lucid.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="embedding-projector" class="anchor" href="#embedding-projector" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://projector.tensorflow.org/" rel="nofollow">Embedding Projector</a>
</h3>
<ul>
<li>我們時常需要將圖片、文字轉成<a href="https://en.wikipedia.org/wiki/Tensor" rel="nofollow">高維數字向量 Embedding</a> 以供神經網路處理，而 Projector 能將此高維向量投影到 2、3 維空間上方便我們理解這些數據</li>
<li>Projector 網站讓你在線上探索幾個常見的資料集，但事實上你也可以<a href="https://www.tensorflow.org/guide/embedding" rel="nofollow">利用 Tensorboard 來視覺化自己的數據</a>。</li>
</ul>
<h3>
<a id="lucid" class="anchor" href="#lucid" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/tensorflow/lucid" rel="nofollow">Lucid</a>
</h3>
<ul>
<li>Lucid 是一個嘗試讓神經網路變得更容易解釋的開源專案，裡頭包含了很多視覺化神經網路的筆記本</li>
<li>你可以直接在 Colab 上執行這些筆記本並了解如何視覺化神經網路</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://paperswithcode.com/" rel="nofollow">Papers with Code</a></th>
<th align="center"><a href="https://pair-code.github.io/what-if-tool/" rel="nofollow">What-If Tool</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://paperswithcode.com/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/papers-with-code.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://pair-code.github.io/what-if-tool/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/what-if-tool.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="papers-with-code" class="anchor" href="#papers-with-code" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://paperswithcode.com/" rel="nofollow">Papers with Code</a>
</h3>
<ul>
<li>將機器學習的學術論文、程式碼實作以及 SOTA 的評價排行榜全部整理匯總在一起的網站，非常適合想要持續追蹤學術及業界最新研究趨勢的人</li>
<li>在這邊可以瀏覽包含電腦視覺、自然語言處理等各大領域在不同任務上表現最好的論文、實作以及資料集</li>
</ul>
<h3>
<a id="what-if-tool" class="anchor" href="#what-if-tool" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://pair-code.github.io/what-if-tool/" rel="nofollow">What-If Tool</a>
</h3>
<ul>
<li>一個與 <a href="#tensorboard">TensorBoard</a> 以及 Jupyter Notebook 整合的探索工具，讓使用者不需寫程式碼就能輕鬆觀察機器學習模型的內部運作以及嘗試各種 What-if 問題（如果 ~ 會怎麼樣？）</li>
<li>基本上就是用來觀察<strong>已訓練</strong>的模型在測試資料集上的表現。利用此工具，使用者可以了解（不僅限於）以下的問題：模型在各類別數據上的表現有無差距？模型是否存在偏見？應該如何調整 Native / Positive False 的比例？</li>
<li>此工具的一大亮點在於讓非專業領域人士也能探索、理解 ML 模型表現。且只要給定模型與資料集, 就不需要每次為了 What-if 問題就寫用過即丟的程式碼</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://github.com/jessevig/bertviz" rel="nofollow">BertViz</a></th>
<th align="center"><a href="https://github.com/dair-ai/ml-visuals" rel="nofollow">ML Visuals</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://github.com/jessevig/bertviz" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/bertviz.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://github.com/dair-ai/ml-visuals" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/ml-visuals.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="bertviz" class="anchor" href="#bertviz" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/jessevig/bertviz" rel="nofollow">BertViz</a>
</h3>
<ul>
<li>BertViz 是一個視覺化自注意力機制的工具，可以用來理解如 <a href="https://arxiv.org/abs/1810.04805" rel="nofollow">BERT</a>、<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">GPT-2</a> 及 <a href="https://arxiv.org/abs/1907.11692" rel="nofollow">RoBERTa</a> 等知名 NLP 模型的內部運作</li>
<li>以下則是幾篇透過 BertViz 來直觀解說 BERT 與 GPT-2 的文章
<ul>
<li><a href="https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html" rel="nofollow">進擊的 BERT：NLP 界的巨人之力與遷移學習</a></li>
<li><a href="https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html" rel="nofollow">直觀理解 GPT-2 語言模型並生成金庸武俠小說</a></li>
</ul>
</li>
</ul>
<h3>
<a id="ml-visuals" class="anchor" href="#ml-visuals" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/dair-ai/ml-visuals" rel="nofollow">ML Visuals</a>
</h3>
<ul>
<li>ML Visuals 是一個社群開源項目，提供超過 100 個常見的機器學習概念 / 深度學習架構圖，可讓任何人在學術論文或是文章直接使用這些圖表。</li>
<li>所有圖表都可以直接從 <a href="https://docs.google.com/presentation/d/11mR1nkIR9fbHegFkcFq8z9oDQ5sjv8E3JJp1LfLGKuk/edit?usp=sharing" rel="nofollow">Google slide</a> 上觀看並使用。建議前往 <a href="https://github.com/dair-ai/ml-visuals" rel="nofollow">Github repo</a> 查看最新版本。</li>
</ul>
<h2>
<a id="其他教材" class="anchor" href="#%E5%85%B6%E4%BB%96%E6%95%99%E6%9D%90" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><div>其他教材</div>
</h2>
<p>除了<a href="#courses">線上課程</a>以外，網路上還有無數的學習資源。</p>
<p>這邊列出一些推薦的深度學習教材，大多數皆以數據科學家常用的 <a href="https://jupyter.org/" rel="nofollow">Jupyter</a> 筆記本的方式呈現。</p>
<p>你可以將感興趣的筆記本導入<a href="#tools">實用工具</a>裡提到的 <a href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="nofollow">Colaboratory（Colab）</a>，馬上開始學習。</p>
<table>
<thead>
<tr>
<th align="center"><a href="https://research.google.com/seedbank/" rel="nofollow">Seedbank</a></th>
<th align="center"><a href="https://github.com/fchollet/deep-learning-with-python-notebooks" rel="nofollow">Deep Learning with Python</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://research.google.com/seedbank/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/seedbank.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://github.com/fchollet/deep-learning-with-python-notebooks" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/fchollet-deep-learning-with-python.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="seedbank" class="anchor" href="#seedbank" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://research.google.com/seedbank/" rel="nofollow">Seedbank</a>
</h3>
<ul>
<li>讓你可以一覽 Colab 上超過 100 個跟機器學習相關的筆記本，並以此為基礎建立各種深度學習應用</li>
<li>熱門筆記本包含<a href="https://research.google.com/seedbank/seed/5695159920492544" rel="nofollow">神經機器翻譯</a>、<a href="https://research.google.com/seedbank/seed/5681034041491456" rel="nofollow">音樂生成</a>以及 <a href="https://research.google.com/seedbank/seed/5631986051842048" rel="nofollow">DeepDream</a>
</li>
<li>因為是 Google 服務，筆記本大多使用 TensorFlow 與 Keras 來實現模型</li>
</ul>
<h3>
<a id="deep-learning-with-python" class="anchor" href="#deep-learning-with-python" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/fchollet/deep-learning-with-python-notebooks" rel="nofollow">Deep Learning with Python</a>
</h3>
<ul>
<li>
<a href="https://keras.io/" rel="nofollow">Keras</a> 作者 <a href="https://ai.google/research/people/105096" rel="nofollow">François Chollet</a> 在 <a href="https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438" rel="nofollow">Deep Learning with Python</a> 一書中用到的所有筆記本。每個筆記本裡頭都清楚地介紹該如何使用 Keras 來實現各種深度學習模型，十分適合第一次使用 Python 實現深度學習的讀者</li>
<li>
<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#top" rel="nofollow">進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南</a>一文的 Keras 程式碼大多基於此</li>
<li>繁體中文的翻譯書籍則為 <a href="https://www.tenlong.com.tw/products/9789863125501?list_name=i-r-zh_tw" rel="nofollow">Deep learning 深度學習必讀 - Keras 大神帶你用 Python 實作</a>
</li>
<li>Keras 在 TensorFlow 2.0 中<a href="https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a" rel="nofollow">為其最重要的高層次 API</a>
</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" rel="nofollow">Stanford CS230 Cheatsheets</a></th>
<th align="center"><a href="https://github.com/madewithml/practicalAI" rel="nofollow">practicalAI</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/cs230-deep-learning-cheatsheet.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://github.com/GokuMohandas/practicalAI" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/practical-ai-pytorch.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="stanford-cs230-cheatsheets" class="anchor" href="#stanford-cs230-cheatsheets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" rel="nofollow">Stanford CS230 Cheatsheets</a>
</h3>
<ul>
<li>史丹佛大學的<a href="http://cs230.stanford.edu/" rel="nofollow">深度學習課程 CS230</a> 釋出的深度學習小抄總結了目前最新的<a href="https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" rel="nofollow">卷積神經網路</a>及<a href="https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="nofollow">循環神經網路</a>知識，還包含了<a href="https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks" rel="nofollow">訓練深度學習時需要使用到的技巧</a>，十分強大</li>
<li>此小抄最適合已經熟悉基礎知識的同學隨時複習運用。你也可以從他們的 <a href="https://github.com/afshinea/stanford-cs-230-deep-learning" rel="nofollow">Github Repo</a> 下載包含上述所有內容的<a href="https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/en/super-cheatsheet-deep-learning.pdf" rel="nofollow">超級 VIP 小抄</a>
</li>
<li>除了深度學習以外，你也可以查看 <a href="https://stanford.edu/%7Eshervine/teaching/cs-229.html" rel="nofollow">CS229 機器學習課程的小抄</a>
</li>
</ul>
<h3>
<a id="practicalai" class="anchor" href="#practicalai" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/madewithml/practicalAI" rel="nofollow">practicalAI</a>
</h3>
<ul>
<li>在 Github 上超過 1 萬星的 Repo。除了深度學習，也有介紹 <a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/basic_ml/01_Python.ipynb" rel="nofollow">Python 基礎</a>及 <a href="https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/basic_ml/03_Pandas.ipynb" rel="nofollow">Pandas</a> 的使用方式</li>
<li>使用 <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> 框架來實現深度學習模型，且所有內容都是 Jupyter 筆記本，可以讓你在 Colab 或本地端執行</li>
</ul>
<hr>
<table>
<thead>
<tr>
<th align="center"><a href="http://demo.allennlp.org/" rel="nofollow">AllenNLP Demo</a></th>
<th align="center"><a href="https://github.com/ageron/handson-ml2" rel="nofollow">Hands-on Machine Learning 2</a></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><a href="http://demo.allennlp.org/" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/allennlp-demo.jpg" style="max-width:100%;"></a></td>
<td align="center"><a href="https://github.com/ageron/handson-ml2" rel="nofollow"><img src="https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/handson-ml2.jpg" style="max-width:100%;"></a></td>
</tr>
</tbody>
</table>
<h3>
<a id="allennlp-demo" class="anchor" href="#allennlp-demo" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="http://demo.allennlp.org/" rel="nofollow">AllenNLP Demo</a>
</h3>
<ul>
<li>清楚地展示了如<a href="https://demo.allennlp.org/machine-comprehension" rel="nofollow">機器理解</a>、<a href="https://demo.allennlp.org/named-entity-recognition" rel="nofollow">命名實體識別</a>等多個自然語言處理任務的情境。每個任務的情境包含了任務所需要的輸入、SOTA 模型的預測結果以及模型內部的注意力機制，對理解一個 NLP 任務的實際應用情境有很大幫助</li>
<li>
<a href="https://allennlp.org/" rel="nofollow">AllenNLP</a> 是一個由 <a href="https://allenai.org/" rel="nofollow">AI2</a> 以 <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> 實現的自然語言處理函式庫</li>
</ul>
<h3>
<a id="hands-on-machine-learning-2" class="anchor" href="#hands-on-machine-learning-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/ageron/handson-ml2" rel="nofollow">Hands-on Machine Learning 2</a>
</h3>
<ul>
<li>前 YouTube 影片分類 PM <a href="https://twitter.com/aureliengeron" rel="nofollow">Aurélien Geron</a> 教你如何透過 Scikit-Learn、Keras 以及 TensorFlow 2 來進行機器學習以及深度學習任務與應用的筆記本彙整。</li>
<li>第二版專注在 TensorFlow 2，其 Github repo 已有超過 6 千顆星，<a href="https://github.com/ageron/handson-ml" rel="nofollow">第一版</a>則有高達 2 萬星。</li>
</ul>
<h2>
<a id="優質文章" class="anchor" href="#%E5%84%AA%E8%B3%AA%E6%96%87%E7%AB%A0" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><div>優質文章</div>
</h2>
<p>這邊列舉了一些幫助我釐清重要概念的部落格以及網站，希望能加速你探索這個深度學習世界。</p>
<p>只要 Google 一下就能發現這些部落格裡頭很多文章都有中文翻譯。但為了尊重原作者，在這邊都列出原文連結。</p>
<ul>
<li>
<a href="https://distill.pub/about/" rel="nofollow">Distill</a>
<ul>
<li>用非常高水準且互動的方式來說明複雜的深度學習概念。<a href="http://www.iro.umontreal.ca/%7Ebengioy/yoshua_en/index.html" rel="nofollow">Yoshua Bengio</a>、<a href="http://www.iangoodfellow.com/" rel="nofollow">Ian Goodfellow</a> 及 <a href="http://cs.stanford.edu/people/karpathy/" rel="nofollow">Andrej Karpathy</a> 等知名人士皆參與其中</li>
</ul>
</li>
<li>
<a href="http://www.r2d3.us/%E5%9C%96%E8%A7%A3%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%AC%AC%E4%B8%80%E7%AB%A0/" rel="nofollow">R2D3: 圖解機器學習</a>
<ul>
<li>利用非常直覺易懂的視覺化來說明機器學習，連結為中文版</li>
</ul>
</li>
<li>
<a href="http://colah.github.io/" rel="nofollow">Christopher Olah's blog</a>
<ul>
<li>詳細解釋不少深度學習概念。作者在<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow">這篇</a>就詳細地解釋了<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%A8%98%E6%86%B6%E5%8A%9B%E5%A5%BD%E7%9A%84-LSTM-%E7%B4%B0%E8%83%9E" rel="nofollow">長短期記憶 LSTM</a> 的概念與變形；在<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" rel="nofollow">這篇</a>則解釋何為 CNN 的卷積運算</li>
</ul>
</li>
<li>
<a href="https://jalammar.github.io/" rel="nofollow">Jay Alammar's blog</a>
<ul>
<li>以清楚易懂的視覺化解釋深度學習概念。<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="nofollow">這篇</a>用大量易懂的動畫說明<a href="https://en.wikipedia.org/wiki/Neural_machine_translation" rel="nofollow">神經機器翻譯</a>，而在<a href="https://jalammar.github.io/illustrated-bert/" rel="nofollow">這篇</a>則介紹如何利用如 <a href="https://allennlp.org/elmo" rel="nofollow">ELMo</a>、<a href="https://github.com/google-research/bert" rel="nofollow">BERT</a> 等預先訓練過的強大模型在自然語言處理進行<a href="https://en.wikipedia.org/wiki/Transfer_learning" rel="nofollow">遷移學習</a>
</li>
</ul>
</li>
<li>
<a href="http://karpathy.github.io/" rel="nofollow">Andrej Karpathy's blog</a>
<ul>
<li>現為 Tesla AI 負責人的 <a href="https://twitter.com/karpathy" rel="nofollow">Andrej Karpathy</a> 在<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="nofollow">這篇</a>明確說明何謂循環神經網路 RNN。文中提供不少應用實例及視覺化來幫助我們理解 RNN 模型究竟學到了什麼，是學習 RNN 的朋友幾乎一定會碰到的一篇文章</li>
</ul>
</li>
</ul>
<h2>
<a id="經典論文" class="anchor" href="#%E7%B6%93%E5%85%B8%E8%AB%96%E6%96%87" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><div>經典論文</div>
</h2>
<p>這邊依發表時間列出深度學習領域的經典 / 重要論文。</p>
<p>為了幫助你快速掌握論文內容以及歷年的研究趨勢，每篇論文下會有非常簡短的介紹（WIP）。</p>
<p>但我們推薦有興趣的人自行閱讀論文以深入了解。</p>
<h3>
<a id="自然語言處理-natural-language-processing-nlp" class="anchor" href="#%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-natural-language-processing-nlp" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>自然語言處理 Natural Language Processing (NLP)</h3>
<ul>
<li><a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="nofollow">2003/02 A Neural Probabilistic Language Model</a></li>
<li><a href="https://arxiv.org/abs/1301.3781" rel="nofollow">2013/01 Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="https://arxiv.org/abs/1308.0850" rel="nofollow">2013/08 Generating Sequences With Recurrent Neural Networks</a></li>
<li><a href="https://arxiv.org/abs/1409.0473" rel="nofollow">2014/09 Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
<li><a href="https://arxiv.org/abs/1508.04025" rel="nofollow">2015/08 Effective Approaches to Attention-based Neural Machine Translation</a></li>
<li>
<a href="https://arxiv.org/abs/1511.01432" rel="nofollow">2015/12 Semi-supervised Sequence Learning</a>
<ul>
<li>推出一套無監督式的預訓練方法。使用無標籤數據訓練後的 RNN 模型在之後的監督式任務表現更好</li>
</ul>
</li>
<li>
<a href="https://arxiv.org/abs/1706.03762" rel="nofollow">2017/06 Attention Is All You Need</a>
<ul>
<li>Google 推出新的神經網路架構 <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="nofollow">Transformer</a>。這個基於自注意力機制的架構特別適合語言理解任務</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1706.05137" rel="nofollow">2017/06 One Model To Learn Them All</a></li>
<li>
<a href="https://arxiv.org/abs/1708.00107" rel="nofollow">2017/08 Learned in Translation: Contextualized Word Vectors</a>
<ul>
<li>監督式預訓練。透過 BiLSTM 與 Encoder-Decoder 架構預先訓練機器翻譯任務並將訓練後的 Encoder 拿來做特徵擷取。將 Encoder 的輸出作為語境向量（Context Vectors, CoVe）處理下游任務</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1801.06146" rel="nofollow">2018/01 Universal Language Model Fine-tuning for Text Classification</a></li>
<li>
<a href="https://arxiv.org/abs/1802.05365" rel="nofollow">2018/02 Deep contextualized word representations</a>
<ul>
<li>
<a href="https://allennlp.org/elmo" rel="nofollow">ELMo 詞向量</a>，利用兩獨立訓練的 LSTM 獲取雙向訊息</li>
</ul>
</li>
<li>
<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="nofollow">2018/06 Improving Language Understanding by Generative Pre-Training</a>
<ul>
<li>
<a href="https://blog.openai.com/language-unsupervised/" rel="nofollow">OpenAI</a> 利用無監督式預訓練以及 Transformer 架構訓練出來的模型表現在多個 NLP 任務表現良好。約使用 8 億詞彙量的資料集</li>
</ul>
</li>
<li>
<a href="https://arxiv.org/abs/1810.04805" rel="nofollow">2018/10 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>
<ul>
<li>Google 暴力美學。利用深層 Transformer 架構、2 個精心設計的預訓練任務以及約 33 億詞彙量的資料集訓練後，得到表現卓越的語言代表模型，打破 11 項 NLP 任務紀錄</li>
</ul>
</li>
<li>
<a href="https://arxiv.org/abs/1905.02450" rel="nofollow">2019/05 MASS: Masked Sequence to Sequence Pre-training for Language Generation</a>
<ul>
<li>Microsoft 利用 Encoder-Decoder 架構以及連續遮罩（consecutive mask）將 BERT 推廣到自然語言生成（NLG）類型任務</li>
</ul>
</li>
<li>
<a href="https://arxiv.org/abs/1905.03197" rel="nofollow">2019/05 Unified Language Model Pre-training for Natural Language Understanding and Generation</a>
<ul>
<li>預訓練階段利用不同遮罩控制 context，同時訓練雙向 LM、單向 LM 以及 Seq2Seq LM。其產生的預訓練模型可以處理 NLU 以及 NLG 任務，並在不加入外部數據的情況下打敗 BERT 在 GLUE 的紀錄</li>
</ul>
</li>
</ul>
<h3>
<a id="電腦視覺-computer-vision-cv" class="anchor" href="#%E9%9B%BB%E8%85%A6%E8%A6%96%E8%A6%BA-computer-vision-cv" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>電腦視覺 Computer Vision (CV)</h3>
<h4>
<a id="類神經網路架構-neural-network-architecture" class="anchor" href="#%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E6%9E%B6%E6%A7%8B-neural-network-architecture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>類神經網路架構 Neural Network Architecture</h4>
<ul>
<li><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="nofollow">1998/01 Gradient-Based Learning Applied to Document Recognition (LeNet-5)</a></li>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="nofollow">2012/12 ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)</a></li>
<li><a href="https://www.cs.toronto.edu/%7Eranzato/publications/taigman_cvpr14.pdf" rel="nofollow">2014/06 DeepFace: Closing the Gap to Human-Level Performance in Face Verification (DeepFace)</a></li>
<li><a href="https://arxiv.org/abs/1409.1556" rel="nofollow">2014/09 Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG)</a></li>
<li><a href="https://arxiv.org/abs/1409.4842" rel="nofollow">2014/09 Goint deeper with convolutions (GoogLeNet)</a></li>
<li><a href="https://arxiv.org/abs/1411.4038" rel="nofollow">2014/11 Fully Convolutional Networks for Semantic Segmentation</a></li>
<li><a href="https://arxiv.org/abs/1505.04597" rel="nofollow">2015/05 U-Net: Convolutional Networks for Biomedical Image Segmentation (U-Net)</a></li>
<li><a href="https://arxiv.org/abs/1512.03385" rel="nofollow">2015/12 Deep Residual Learning for Image Recognition (ResNet)</a></li>
<li><a href="https://arxiv.org/abs/1704.04861" rel="nofollow">2017/04 MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (MobileNets)</a></li>
<li><a href="https://arxiv.org/abs/1707.01083" rel="nofollow">2017/07 ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices (ShuffleNet)</a></li>
</ul>
<h4>
<a id="資料集-dataset" class="anchor" href="#%E8%B3%87%E6%96%99%E9%9B%86-dataset" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>資料集 Dataset</h4>
<ul>
<li><a href="http://www.image-net.org/papers/imagenet_cvpr09.pdf" rel="nofollow">2009/06 ImageNet: A Large-Scale Hierarchical Image Database (ImageNet)</a></li>
</ul>
<h4>
<a id="物體偵測與切割-object-detection-and-segmentation" class="anchor" href="#%E7%89%A9%E9%AB%94%E5%81%B5%E6%B8%AC%E8%88%87%E5%88%87%E5%89%B2-object-detection-and-segmentation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>物體偵測與切割 Object Detection and Segmentation</h4>
<ul>
<li><a href="https://arxiv.org/abs/1311.2524" rel="nofollow">2013/11 Rich feature hierarchies for accurate object detection and semantic segmentation (R-CNN)</a></li>
<li><a href="https://arxiv.org/abs/1312.6229" rel="nofollow">2013/12 OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks (OverFeat)</a></li>
<li><a href="https://arxiv.org/abs/1504.08083" rel="nofollow">2015/04 Fast R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1506.01497" rel="nofollow">2015/06 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (Faster R-CNN)</a></li>
<li><a href="https://arxiv.org/abs/1506.02640" rel="nofollow">2015/06 You Only Look Once: Unified, Real-Time Object Detection (YOLO)</a></li>
<li><a href="https://arxiv.org/abs/1512.02325" rel="nofollow">2015/12 SSD: Single Shot MultiBox Detector (SSD)</a></li>
<li><a href="https://arxiv.org/abs/1612.08242" rel="nofollow">2016/12 YOLO9000: Better, Faster, Stronger (YOLOv2)</a></li>
<li><a href="https://arxiv.org/abs/1703.06870" rel="nofollow">2017/03 Mask R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1804.02767" rel="nofollow">2018/04 YOLOv3: An Incremental Improvement (YOLOv3)</a></li>
</ul>
<h4>
<a id="生成模型-generative-models" class="anchor" href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B-generative-models" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>生成模型 Generative Models</h4>
<ul>
<li><a href="https://arxiv.org/abs/1406.2661" rel="nofollow">2014/06 Generative Adversarial Networks (GAN)</a></li>
<li><a href="https://arxiv.org/abs/1511.06434" rel="nofollow">2015/13 Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)</a></li>
<li><a href="https://arxiv.org/abs/1701.07875" rel="nofollow">2017/01 Wasserstein GAN (WGAN)</a></li>
<li><a href="https://arxiv.org/abs/1703.10593" rel="nofollow">2017/03 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN)</a></li>
</ul>
<h2>
<a id="其他整理" class="anchor" href="#%E5%85%B6%E4%BB%96%E6%95%B4%E7%90%86" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><div>其他整理</div>
</h2>
<p>這邊列出其他優質的資源整理網站 / Github Repo，供你繼續探索深度學習。</p>
<h3>
<a id="deep-learning-ocean" class="anchor" href="#deep-learning-ocean" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a><a href="https://github.com/osforscience/deep-learning-ocean" rel="nofollow">deep-learning-ocean</a>
</h3>
<ul>
<li>整理了不少深度學習資源，但最值得參考的是數據集以及論文的分類整理。</li>
</ul>
<h2>
<a id="待辦事項" class="anchor" href="#%E5%BE%85%E8%BE%A6%E4%BA%8B%E9%A0%85" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>待辦事項</h2>
<p>還有不少內容正在整理，以下是目前我們打算增加的一些項目：</p>
<ul>
<li>深度學習中英術語對照表</li>
<li>值得追蹤的業界 / 學界影響人物清單</li>
<li>無圖的資源列表版本</li>
<li>一些 Jupyter Notebook 範例</li>
</ul>
<p>而我們也會持續將新資源加入如<a href="#tools">實用工具</a>、<a href="#blogs">優質文章</a>等列表裡頭。</p>
<h2>
<a id="如何貢獻" class="anchor" href="#%E5%A6%82%E4%BD%95%E8%B2%A2%E7%8D%BB" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>如何貢獻</h2>
<p>非常歡迎你一起加入改善這個 Repo，讓更多人有方向地學習 Deep Learning：）</p>
<p>如果你有</p>
<ul>
<li>其他值得推薦的深度學習資源</li>
<li>針對此 Repo 內容的改善建議</li>
<li>其他任何你想得到的東西</li>
</ul>
<p>都歡迎你<a href="https://github.com/leemengtaiwan/deep-learning-resources/issues/new" rel="nofollow">提出新的 Issue</a> 來讓我們知道。</p>
<p>如果是想增加新資源的話，只附上連結也是沒有問題的，謝謝！</p>
<script type="text/javascript">(function(){window['__CF$cv$params']={r:'6120bca52c45e738',m:'cbf9a0f4ebec4460e20d9f39f36e8557d55c0893-1610725663-1800-AYMtmekplVcdV5yHc95BEzNq4x7aHM1jUPfMvl2rn1J5/91WexgF/Iw2UA0sKC2gk/Gko9Vv77CROCAI0lvCQNoWINWawMyTFPRsAcZDTFESOApo1joPgRgfeELOwEoxxle4Pe03xqQJNYmCHeVSz2UeCziZjYScARz9chLXS6L/',s:[0x2d0011f590,0xc74d411663],}})();</script></body></html>
</div><div><br/><div style="border:1px solid gray;border-radius:5px;background-color:lightgray;padding:2%"><div style="font-size:24px;font-weight:bold;margin-bottom:10px">Get A Weekly Email With Trending Projects For These Topics</div><div style="margin-bottom:10px">No Spam.  Unsubscribe easily at any time.</div><div><div><input type="checkbox" name="topics" value="python"/> <!-- -->python<!-- --> (<!-- -->49,729<!-- -->) </div><div><input type="checkbox" name="topics" value="jupyter-notebook"/> <!-- -->jupyter-notebook<!-- --> (<!-- -->5,677<!-- -->) </div><div><input type="checkbox" name="topics" value="deep-learning"/> <!-- -->deep-learning<!-- --> (<!-- -->3,659<!-- -->) </div><div><input type="checkbox" name="topics" value="pytorch"/> <!-- -->pytorch<!-- --> (<!-- -->2,132<!-- -->) </div><div><input type="checkbox" name="topics" value="tensorflow"/> <!-- -->tensorflow<!-- --> (<!-- -->2,063<!-- -->) </div><div><input type="checkbox" name="topics" value="neural-networks"/> <!-- -->neural-networks<!-- --> (<!-- -->401<!-- -->) </div><div><input type="checkbox" name="topics" value="tools"/> <!-- -->tools<!-- --> (<!-- -->318<!-- -->) </div><div><input type="checkbox" name="topics" value="gan"/> <!-- -->gan<!-- --> (<!-- -->314<!-- -->) </div><div><input type="checkbox" name="topics" value="deeplearning"/> <!-- -->deeplearning<!-- --> (<!-- -->270<!-- -->) </div><div><input type="checkbox" name="topics" value="generative-adversarial-network"/> <!-- -->generative-adversarial-network<!-- --> (<!-- -->234<!-- -->) </div><div><input type="checkbox" name="topics" value="chinese"/> <!-- -->chinese<!-- --> (<!-- -->187<!-- -->) </div><div><input type="checkbox" name="topics" value="collections"/> <!-- -->collections<!-- --> (<!-- -->60<!-- -->) </div><div><input type="checkbox" name="topics" value="courses"/> <!-- -->courses<!-- --> (<!-- -->33<!-- -->) </div></div><div><input type="email" style="width:95%;margin-top:10px;margin-bottom:10px;padding:5px;font-size:20px;border-radius:5px" name="email" placeholder="Your email address"/></div><div><input type="submit" style="color:white;background-color:#0366d6;font-size:20px;border:1px solid black;border-radius:5px;padding:5px;cursor:pointer" value="Subscribe"/></div></div></div><h2>Find Open Source By Browsing 7,000 Topics Across 59 Categories</h2><div class="aos_topics_sidebar" style="margin:0;width:100%"><div class="category_topic_filter_container"><input style="width:100%" placeholder="Search, browse and combine topics" value="" class="aos_filter_topics_input"/></div><div style="display:block;box-shadow:1px 1px 5px lightgray"><a data-category-name="Advertising" href="https://awesomeopensource.com/categories/advertising"><div class="aos_category_explorer_category_container" data-category-name="Advertising"><span data-category-name="Advertising">Advertising<span data-category-name="Advertising" class="aos_project_count"> 📦 <!-- -->10</span></span></div></a><a data-category-name="All Projects" href="https://awesomeopensource.com/projects"><div class="aos_category_explorer_category_container" data-category-name="All Projects"><span data-category-name="All Projects">All Projects</span></div></a><a data-category-name="Application Programming Interfaces" href="https://awesomeopensource.com/categories/application-programming-interfaces"><div class="aos_category_explorer_category_container" data-category-name="Application Programming Interfaces"><span data-category-name="Application Programming Interfaces">Application Programming Interfaces<span data-category-name="Application Programming Interfaces" class="aos_project_count"> 📦 <!-- -->124</span></span></div></a><a data-category-name="Applications" href="https://awesomeopensource.com/categories/applications"><div class="aos_category_explorer_category_container" data-category-name="Applications"><span data-category-name="Applications">Applications<span data-category-name="Applications" class="aos_project_count"> 📦 <!-- -->192</span></span></div></a><a data-category-name="Artificial Intelligence" href="https://awesomeopensource.com/categories/artificial-intelligence"><div class="aos_category_explorer_category_container" data-category-name="Artificial Intelligence"><span data-category-name="Artificial Intelligence">Artificial Intelligence<span data-category-name="Artificial Intelligence" class="aos_project_count"> 📦 <!-- -->78</span></span></div></a><a data-category-name="Blockchain" href="https://awesomeopensource.com/categories/blockchain"><div class="aos_category_explorer_category_container" data-category-name="Blockchain"><span data-category-name="Blockchain">Blockchain<span data-category-name="Blockchain" class="aos_project_count"> 📦 <!-- -->73</span></span></div></a><a data-category-name="Build Tools" href="https://awesomeopensource.com/categories/build-tools"><div class="aos_category_explorer_category_container" data-category-name="Build Tools"><span data-category-name="Build Tools">Build Tools<span data-category-name="Build Tools" class="aos_project_count"> 📦 <!-- -->113</span></span></div></a><a data-category-name="Cloud Computing" href="https://awesomeopensource.com/categories/cloud-computing"><div class="aos_category_explorer_category_container" data-category-name="Cloud Computing"><span data-category-name="Cloud Computing">Cloud Computing<span data-category-name="Cloud Computing" class="aos_project_count"> 📦 <!-- -->80</span></span></div></a><a data-category-name="Code Quality" href="https://awesomeopensource.com/categories/code-quality"><div class="aos_category_explorer_category_container" data-category-name="Code Quality"><span data-category-name="Code Quality">Code Quality<span data-category-name="Code Quality" class="aos_project_count"> 📦 <!-- -->28</span></span></div></a><a data-category-name="Collaboration" href="https://awesomeopensource.com/categories/collaboration"><div class="aos_category_explorer_category_container" data-category-name="Collaboration"><span data-category-name="Collaboration">Collaboration<span data-category-name="Collaboration" class="aos_project_count"> 📦 <!-- -->32</span></span></div></a><a data-category-name="Command Line Interface" href="https://awesomeopensource.com/categories/command-line-interface"><div class="aos_category_explorer_category_container" data-category-name="Command Line Interface"><span data-category-name="Command Line Interface">Command Line Interface<span data-category-name="Command Line Interface" class="aos_project_count"> 📦 <!-- -->49</span></span></div></a><a data-category-name="Community" href="https://awesomeopensource.com/categories/community"><div class="aos_category_explorer_category_container" data-category-name="Community"><span data-category-name="Community">Community<span data-category-name="Community" class="aos_project_count"> 📦 <!-- -->83</span></span></div></a><a data-category-name="Companies" href="https://awesomeopensource.com/categories/companies"><div class="aos_category_explorer_category_container" data-category-name="Companies"><span data-category-name="Companies">Companies<span data-category-name="Companies" class="aos_project_count"> 📦 <!-- -->60</span></span></div></a><a data-category-name="Compilers" href="https://awesomeopensource.com/categories/compilers"><div class="aos_category_explorer_category_container" data-category-name="Compilers"><span data-category-name="Compilers">Compilers<span data-category-name="Compilers" class="aos_project_count"> 📦 <!-- -->63</span></span></div></a><a data-category-name="Computer Science" href="https://awesomeopensource.com/categories/computer-science"><div class="aos_category_explorer_category_container" data-category-name="Computer Science"><span data-category-name="Computer Science">Computer Science<span data-category-name="Computer Science" class="aos_project_count"> 📦 <!-- -->80</span></span></div></a><a data-category-name="Configuration Management" href="https://awesomeopensource.com/categories/configuration-management"><div class="aos_category_explorer_category_container" data-category-name="Configuration Management"><span data-category-name="Configuration Management">Configuration Management<span data-category-name="Configuration Management" class="aos_project_count"> 📦 <!-- -->42</span></span></div></a><a data-category-name="Content Management" href="https://awesomeopensource.com/categories/content-management"><div class="aos_category_explorer_category_container" data-category-name="Content Management"><span data-category-name="Content Management">Content Management<span data-category-name="Content Management" class="aos_project_count"> 📦 <!-- -->175</span></span></div></a><a data-category-name="Control Flow" href="https://awesomeopensource.com/categories/control-flow"><div class="aos_category_explorer_category_container" data-category-name="Control Flow"><span data-category-name="Control Flow">Control Flow<span data-category-name="Control Flow" class="aos_project_count"> 📦 <!-- -->213</span></span></div></a><a data-category-name="Data Formats" href="https://awesomeopensource.com/categories/data-formats"><div class="aos_category_explorer_category_container" data-category-name="Data Formats"><span data-category-name="Data Formats">Data Formats<span data-category-name="Data Formats" class="aos_project_count"> 📦 <!-- -->78</span></span></div></a><a data-category-name="Data Processing" href="https://awesomeopensource.com/categories/data-processing"><div class="aos_category_explorer_category_container" data-category-name="Data Processing"><span data-category-name="Data Processing">Data Processing<span data-category-name="Data Processing" class="aos_project_count"> 📦 <!-- -->276</span></span></div></a><a data-category-name="Data Storage" href="https://awesomeopensource.com/categories/data-storage"><div class="aos_category_explorer_category_container" data-category-name="Data Storage"><span data-category-name="Data Storage">Data Storage<span data-category-name="Data Storage" class="aos_project_count"> 📦 <!-- -->135</span></span></div></a><a data-category-name="Economics" href="https://awesomeopensource.com/categories/economics"><div class="aos_category_explorer_category_container" data-category-name="Economics"><span data-category-name="Economics">Economics<span data-category-name="Economics" class="aos_project_count"> 📦 <!-- -->64</span></span></div></a><a data-category-name="Frameworks" href="https://awesomeopensource.com/categories/frameworks"><div class="aos_category_explorer_category_container" data-category-name="Frameworks"><span data-category-name="Frameworks">Frameworks<span data-category-name="Frameworks" class="aos_project_count"> 📦 <!-- -->215</span></span></div></a><a data-category-name="Games" href="https://awesomeopensource.com/categories/games"><div class="aos_category_explorer_category_container" data-category-name="Games"><span data-category-name="Games">Games<span data-category-name="Games" class="aos_project_count"> 📦 <!-- -->129</span></span></div></a><a data-category-name="Graphics" href="https://awesomeopensource.com/categories/graphics"><div class="aos_category_explorer_category_container" data-category-name="Graphics"><span data-category-name="Graphics">Graphics<span data-category-name="Graphics" class="aos_project_count"> 📦 <!-- -->110</span></span></div></a><a data-category-name="Hardware" href="https://awesomeopensource.com/categories/hardware"><div class="aos_category_explorer_category_container" data-category-name="Hardware"><span data-category-name="Hardware">Hardware<span data-category-name="Hardware" class="aos_project_count"> 📦 <!-- -->152</span></span></div></a><a data-category-name="Integrated Development Environments" href="https://awesomeopensource.com/categories/integrated-development-environments"><div class="aos_category_explorer_category_container" data-category-name="Integrated Development Environments"><span data-category-name="Integrated Development Environments">Integrated Development Environments<span data-category-name="Integrated Development Environments" class="aos_project_count"> 📦 <!-- -->49</span></span></div></a><a data-category-name="Learning Resources" href="https://awesomeopensource.com/categories/learning-resources"><div class="aos_category_explorer_category_container" data-category-name="Learning Resources"><span data-category-name="Learning Resources">Learning Resources<span data-category-name="Learning Resources" class="aos_project_count"> 📦 <!-- -->166</span></span></div></a><a data-category-name="Legal" href="https://awesomeopensource.com/categories/legal"><div class="aos_category_explorer_category_container" data-category-name="Legal"><span data-category-name="Legal">Legal<span data-category-name="Legal" class="aos_project_count"> 📦 <!-- -->29</span></span></div></a><a data-category-name="Libraries" href="https://awesomeopensource.com/categories/libraries"><div class="aos_category_explorer_category_container" data-category-name="Libraries"><span data-category-name="Libraries">Libraries<span data-category-name="Libraries" class="aos_project_count"> 📦 <!-- -->129</span></span></div></a><a data-category-name="Lists Of Projects" href="https://awesomeopensource.com/categories/lists-of-projects"><div class="aos_category_explorer_category_container" data-category-name="Lists Of Projects"><span data-category-name="Lists Of Projects">Lists Of Projects<span data-category-name="Lists Of Projects" class="aos_project_count"> 📦 <!-- -->22</span></span></div></a><a data-category-name="Machine Learning" href="https://awesomeopensource.com/categories/machine-learning"><div class="aos_category_explorer_category_container" data-category-name="Machine Learning"><span data-category-name="Machine Learning">Machine Learning<span data-category-name="Machine Learning" class="aos_project_count"> 📦 <!-- -->347</span></span></div></a><a data-category-name="Mapping" href="https://awesomeopensource.com/categories/mapping"><div class="aos_category_explorer_category_container" data-category-name="Mapping"><span data-category-name="Mapping">Mapping<span data-category-name="Mapping" class="aos_project_count"> 📦 <!-- -->64</span></span></div></a><a data-category-name="Marketing" href="https://awesomeopensource.com/categories/marketing"><div class="aos_category_explorer_category_container" data-category-name="Marketing"><span data-category-name="Marketing">Marketing<span data-category-name="Marketing" class="aos_project_count"> 📦 <!-- -->15</span></span></div></a><a data-category-name="Mathematics" href="https://awesomeopensource.com/categories/mathematics"><div class="aos_category_explorer_category_container" data-category-name="Mathematics"><span data-category-name="Mathematics">Mathematics<span data-category-name="Mathematics" class="aos_project_count"> 📦 <!-- -->55</span></span></div></a><a data-category-name="Media" href="https://awesomeopensource.com/categories/media"><div class="aos_category_explorer_category_container" data-category-name="Media"><span data-category-name="Media">Media<span data-category-name="Media" class="aos_project_count"> 📦 <!-- -->239</span></span></div></a><a data-category-name="Messaging" href="https://awesomeopensource.com/categories/messaging"><div class="aos_category_explorer_category_container" data-category-name="Messaging"><span data-category-name="Messaging">Messaging<span data-category-name="Messaging" class="aos_project_count"> 📦 <!-- -->98</span></span></div></a><a data-category-name="Networking" href="https://awesomeopensource.com/categories/networking"><div class="aos_category_explorer_category_container" data-category-name="Networking"><span data-category-name="Networking">Networking<span data-category-name="Networking" class="aos_project_count"> 📦 <!-- -->315</span></span></div></a><a data-category-name="Operating Systems" href="https://awesomeopensource.com/categories/operating-systems"><div class="aos_category_explorer_category_container" data-category-name="Operating Systems"><span data-category-name="Operating Systems">Operating Systems<span data-category-name="Operating Systems" class="aos_project_count"> 📦 <!-- -->89</span></span></div></a><a data-category-name="Operations" href="https://awesomeopensource.com/categories/operations"><div class="aos_category_explorer_category_container" data-category-name="Operations"><span data-category-name="Operations">Operations<span data-category-name="Operations" class="aos_project_count"> 📦 <!-- -->121</span></span></div></a><a data-category-name="Package Managers" href="https://awesomeopensource.com/categories/package-managers"><div class="aos_category_explorer_category_container" data-category-name="Package Managers"><span data-category-name="Package Managers">Package Managers<span data-category-name="Package Managers" class="aos_project_count"> 📦 <!-- -->55</span></span></div></a><a data-category-name="Programming Languages" href="https://awesomeopensource.com/categories/programming-languages"><div class="aos_category_explorer_category_container" data-category-name="Programming Languages"><span data-category-name="Programming Languages">Programming Languages<span data-category-name="Programming Languages" class="aos_project_count"> 📦 <!-- -->245</span></span></div></a><a data-category-name="Runtime Environments" href="https://awesomeopensource.com/categories/runtime-environments"><div class="aos_category_explorer_category_container" data-category-name="Runtime Environments"><span data-category-name="Runtime Environments">Runtime Environments<span data-category-name="Runtime Environments" class="aos_project_count"> 📦 <!-- -->100</span></span></div></a><a data-category-name="Science" href="https://awesomeopensource.com/categories/science"><div class="aos_category_explorer_category_container" data-category-name="Science"><span data-category-name="Science">Science<span data-category-name="Science" class="aos_project_count"> 📦 <!-- -->42</span></span></div></a><a data-category-name="Security" href="https://awesomeopensource.com/categories/security"><div class="aos_category_explorer_category_container" data-category-name="Security"><span data-category-name="Security">Security<span data-category-name="Security" class="aos_project_count"> 📦 <!-- -->396</span></span></div></a><a data-category-name="Social Media" href="https://awesomeopensource.com/categories/social-media"><div class="aos_category_explorer_category_container" data-category-name="Social Media"><span data-category-name="Social Media">Social Media<span data-category-name="Social Media" class="aos_project_count"> 📦 <!-- -->27</span></span></div></a><a data-category-name="Software Architecture" href="https://awesomeopensource.com/categories/software-architecture"><div class="aos_category_explorer_category_container" data-category-name="Software Architecture"><span data-category-name="Software Architecture">Software Architecture<span data-category-name="Software Architecture" class="aos_project_count"> 📦 <!-- -->72</span></span></div></a><a data-category-name="Software Development" href="https://awesomeopensource.com/categories/software-development"><div class="aos_category_explorer_category_container" data-category-name="Software Development"><span data-category-name="Software Development">Software Development<span data-category-name="Software Development" class="aos_project_count"> 📦 <!-- -->72</span></span></div></a><a data-category-name="Software Performance" href="https://awesomeopensource.com/categories/software-performance"><div class="aos_category_explorer_category_container" data-category-name="Software Performance"><span data-category-name="Software Performance">Software Performance<span data-category-name="Software Performance" class="aos_project_count"> 📦 <!-- -->58</span></span></div></a><a data-category-name="Software Quality" href="https://awesomeopensource.com/categories/software-quality"><div class="aos_category_explorer_category_container" data-category-name="Software Quality"><span data-category-name="Software Quality">Software Quality<span data-category-name="Software Quality" class="aos_project_count"> 📦 <!-- -->133</span></span></div></a><a data-category-name="Text Editors" href="https://awesomeopensource.com/categories/text-editors"><div class="aos_category_explorer_category_container" data-category-name="Text Editors"><span data-category-name="Text Editors">Text Editors<span data-category-name="Text Editors" class="aos_project_count"> 📦 <!-- -->49</span></span></div></a><a data-category-name="Text Processing" href="https://awesomeopensource.com/categories/text-processing"><div class="aos_category_explorer_category_container" data-category-name="Text Processing"><span data-category-name="Text Processing">Text Processing<span data-category-name="Text Processing" class="aos_project_count"> 📦 <!-- -->136</span></span></div></a><a data-category-name="User Interface" href="https://awesomeopensource.com/categories/user-interface"><div class="aos_category_explorer_category_container" data-category-name="User Interface"><span data-category-name="User Interface">User Interface<span data-category-name="User Interface" class="aos_project_count"> 📦 <!-- -->330</span></span></div></a><a data-category-name="User Interface Components" href="https://awesomeopensource.com/categories/user-interface-components"><div class="aos_category_explorer_category_container" data-category-name="User Interface Components"><span data-category-name="User Interface Components">User Interface Components<span data-category-name="User Interface Components" class="aos_project_count"> 📦 <!-- -->514</span></span></div></a><a data-category-name="Version Control" href="https://awesomeopensource.com/categories/version-control"><div class="aos_category_explorer_category_container" data-category-name="Version Control"><span data-category-name="Version Control">Version Control<span data-category-name="Version Control" class="aos_project_count"> 📦 <!-- -->30</span></span></div></a><a data-category-name="Virtualization" href="https://awesomeopensource.com/categories/virtualization"><div class="aos_category_explorer_category_container" data-category-name="Virtualization"><span data-category-name="Virtualization">Virtualization<span data-category-name="Virtualization" class="aos_project_count"> 📦 <!-- -->71</span></span></div></a><a data-category-name="Web Browsers" href="https://awesomeopensource.com/categories/web-browsers"><div class="aos_category_explorer_category_container" data-category-name="Web Browsers"><span data-category-name="Web Browsers">Web Browsers<span data-category-name="Web Browsers" class="aos_project_count"> 📦 <!-- -->42</span></span></div></a><a data-category-name="Web Servers" href="https://awesomeopensource.com/categories/web-servers"><div class="aos_category_explorer_category_container" data-category-name="Web Servers"><span data-category-name="Web Servers">Web Servers<span data-category-name="Web Servers" class="aos_project_count"> 📦 <!-- -->26</span></span></div></a><a data-category-name="Web User Interface" href="https://awesomeopensource.com/categories/web-user-interface"><div class="aos_category_explorer_category_container" data-category-name="Web User Interface"><span data-category-name="Web User Interface">Web User Interface<span data-category-name="Web User Interface" class="aos_project_count"> 📦 <!-- -->210</span></span></div></a></div></div><div class="aos_project_metadata_trademark_notice">&quot;<!-- -->Deep <!-- -->Learning <!-- -->Resources<!-- -->&quot; and other potentially trademarked words, copyrighted images and copyrighted readme contents likely belong to the legal entity who owns the &quot;<a href="https://github.com/leemengtaiwan" target="_blank">Leemengtaiwan</a>&quot; organization.  Awesome Open Source is not affiliated with the legal entity who owns the &quot;<a href="https://github.com/leemengtaiwan" target="_blank">Leemengtaiwan</a>&quot; organization.</div></div></div><div class="aos_fixed_footer_container"><br/></div></div></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="Root" data-dom-id="Root-react-component-ceaf6eed-261c-49b7-948b-803511ff3bf3">{"topics":[{"id":null,"name":"javascript","count":67310},{"id":null,"name":"python","count":49729},{"id":null,"name":"java","count":30048},{"id":null,"name":"c-plus-plus","count":17339},{"id":null,"name":"php","count":15812},{"id":null,"name":"c","count":14340},{"id":null,"name":"go","count":14132},{"id":null,"name":"ruby","count":12268},{"id":null,"name":"c-sharp","count":11505},{"id":null,"name":"objective-c","count":10769}],"projectTopics":[{"id":null,"name":"chinese","count":187},{"id":null,"name":"collections","count":60},{"id":null,"name":"courses","count":33},{"id":null,"name":"deeplearning","count":270},{"id":null,"name":"deep-learning","count":3659},{"id":null,"name":"gan","count":314},{"id":null,"name":"generative-adversarial-network","count":234},{"id":null,"name":"jupyter-notebook","count":5677},{"id":null,"name":"neural-networks","count":401},{"id":null,"name":"python","count":49729},{"id":null,"name":"pytorch","count":2132},{"id":null,"name":"tensorflow","count":2063},{"id":null,"name":"tools","count":318}],"selectedTopics":[],"currentUrl":"https://awesomeopensource.com/project/leemengtaiwan/deep-learning-resources","projects":[],"project":{"id":140739,"created_at":"2019-01-31T23:38:34.054Z","updated_at":"2021-01-13T18:20:05.029Z","markdown_readme":"\u003c!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\"\u003e\n\u003chtml\u003e\u003cbody\u003e\n\u003cdiv align=\"center\"\u003e\n  \u003ca href=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/general/paper-ball.jpg\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/general/paper-ball.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/div\u003e\n\u003chr\u003e\n\u003cp\u003e這裡紀錄了我在學習\u003ca href=\"https://leemeng.tw/deep-learning-for-everyone-understand-neural-net-and-linear-algebra.html\" rel=\"nofollow\"\u003e深度學習\u003c/a\u003e時蒐集的一些線上資源。內容由淺入深，而且會不斷更新，希望能幫助你順利地開始學習：）\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"本文章節\" class=\"anchor\" href=\"#%E6%9C%AC%E6%96%87%E7%AB%A0%E7%AF%80\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e本文章節\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#playground\"\u003e遊玩空間\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#courses\"\u003e線上課程\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#tools\"\u003e實用工具\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#tutorials\"\u003e其他教材\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#blogs\"\u003e優質文章\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#papers\"\u003e經典論文\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#collections\"\u003e其他整理\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"遊玩空間\" class=\"anchor\" href=\"#%E9%81%8A%E7%8E%A9%E7%A9%BA%E9%96%93\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cdiv\u003e遊玩空間\u003c/div\u003e\n\u003c/h2\u003e\n\u003cp\u003e這節列舉了一些透過瀏覽器就能馬上開始遊玩 / 體驗深度學習的應用。作為這些應用的使用者，你可以先高層次、直觀地了解深度學習能做些什麼。之後有興趣再進一步了解背後原理。\u003c/p\u003e\n\u003cp\u003e這小節最適合：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e想要快速體會深度學習如何被應用在真實世界的好奇寶寶\u003c/li\u003e\n\u003cli\u003e想要直觀理解\u003ca href=\"https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\" rel=\"nofollow\"\u003e類神經網路（Artifical Neural Network）\u003c/a\u003e運作方式的人\u003c/li\u003e\n\u003cli\u003e想從別人的深度學習應用取得一些靈感的開發者\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://playground.tensorflow.org/\" rel=\"nofollow\"\u003eDeep Playground\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://cs.stanford.edu/people/karpathy/convnetjs/index.html\" rel=\"nofollow\"\u003eConvNetJS\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://playground.tensorflow.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/deep-playground.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://cs.stanford.edu/people/karpathy/convnetjs/index.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/convnetjs.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"deep-playground\" class=\"anchor\" href=\"#deep-playground\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://playground.tensorflow.org/\" rel=\"nofollow\"\u003eDeep Playground\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e由 \u003ca href=\"https://github.com/tensorflow/playground\" rel=\"nofollow\"\u003eTensorflow 團隊\u003c/a\u003e推出，模擬訓練一個類神經網路的過程並了解其運作原理\u003c/li\u003e\n\u003cli\u003e可以搭配這篇 \u003ca href=\"https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/playground-exercises\" rel=\"nofollow\"\u003eIntroduction to Neural Networks: Playground Exercises\u003c/a\u003e 學習\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"convnetjs\" class=\"anchor\" href=\"#convnetjs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://cs.stanford.edu/people/karpathy/convnetjs/\" rel=\"nofollow\"\u003eConvNetJS\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e訓練類神經網路來解決經典的 \u003ca href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html\" rel=\"nofollow\"\u003eMNIST 手寫數字辨識問題\u003c/a\u003e、\u003ca href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html\" rel=\"nofollow\"\u003e圖片生成\u003c/a\u003e以及\u003ca href=\"https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html\" rel=\"nofollow\"\u003e增強式學習\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e由 Tesla 的 AI 負責人 \u003ca href=\"https://cs.stanford.edu/people/karpathy/\" rel=\"nofollow\"\u003eAndrej Karpathy\u003c/a\u003e 建立\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://magenta.tensorflow.org/\" rel=\"nofollow\"\u003eMagenta\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://experiments.withgoogle.com/collection/ai\" rel=\"nofollow\"\u003eGoogle AI Experiments\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://magenta.tensorflow.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/magenta.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://experiments.withgoogle.com/collection/ai\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/google-ai-experiment.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"magenta\" class=\"anchor\" href=\"#magenta\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://magenta.tensorflow.org/\" rel=\"nofollow\"\u003eMagenta\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e一個利用\u003ca href=\"https://zh.wikipedia.org/zh-hant/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\" rel=\"nofollow\"\u003e機器學習\u003c/a\u003e來協助人們進行音樂以及藝術創作的開源專案\u003c/li\u003e\n\u003cli\u003e可以在網站上的 \u003ca href=\"https://magenta.tensorflow.org/demos\" rel=\"nofollow\"\u003eDemo 頁面\u003c/a\u003e嘗試各種由深度學習驅動的音樂 / 繪畫應用（如彈奏鋼琴、擊鼓）\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"google-ai-experiments\" class=\"anchor\" href=\"#google-ai-experiments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://experiments.withgoogle.com/collection/ai\" rel=\"nofollow\"\u003eGoogle AI Experiments\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e這邊展示了接近 40 個利用圖片、語言以及音樂來與使用者產生互動的機器學習 Apps，值得慢慢探索\u003c/li\u003e\n\u003cli\u003e知名例子有 \u003ca href=\"https://quickdraw.withgoogle.com/\" rel=\"nofollow\"\u003eQuick Draw\u003c/a\u003e 以及 \u003ca href=\"https://teachablemachine.withgoogle.com/\" rel=\"nofollow\"\u003eTeachable Machine\u003c/a\u003e，將在下方介紹\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://quickdraw.withgoogle.com/\" rel=\"nofollow\"\u003eQuick Draw\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://teachablemachine.withgoogle.com/\" rel=\"nofollow\"\u003eTeachable Machine\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://quickdraw.withgoogle.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/quickdraw.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://teachablemachine.withgoogle.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/teachable-machine.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"quick-draw\" class=\"anchor\" href=\"#quick-draw\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://quickdraw.withgoogle.com/\" rel=\"nofollow\"\u003eQuick Draw\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e由 Google 推出的知名手寫塗鴉辨識，使用的神經網路架構有常見的\u003ca href=\"https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C\" rel=\"nofollow\"\u003e卷積神經網路 CNN \u003c/a\u003e以及\u003ca href=\"https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E6%9C%89%E8%A8%98%E6%86%B6%E7%9A%84%E5%BE%AA%E7%92%B0%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF_1\" rel=\"nofollow\"\u003e循環神經網路 RNN\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e該深度學習模型會不斷將最新的筆觸當作輸入來預測使用者想畫的物件。你會驚嘆於她精準且即時的判斷\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"teachable-machine\" class=\"anchor\" href=\"#teachable-machine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://teachablemachine.withgoogle.com/\" rel=\"nofollow\"\u003eTeachable Machine\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e利用電腦 / 手機上的相機來訓練能將影像對應到其他圖片、音訊的神經網路，饒富趣味\u003c/li\u003e\n\u003cli\u003e透過這例子，你將暸解機器學習的神奇之處以及其侷限所在\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://tenso.rs/demos/fast-neural-style/\" rel=\"nofollow\"\u003eFast Neural Style\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://js.tensorflow.org/\" rel=\"nofollow\"\u003eTensorFlow.js\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://tenso.rs/demos/fast-neural-style/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/fast-neural-style.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://js.tensorflow.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/human-pose-estimation.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"fast-neural-style\" class=\"anchor\" href=\"#fast-neural-style\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://tenso.rs/demos/fast-neural-style/\" rel=\"nofollow\"\u003eFast Neural Style\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e展示如何使用 WebGL 在瀏覽器快速地進行\u003ca href=\"https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398\" rel=\"nofollow\"\u003e神經風格轉換 Neural Style Transfer\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e你可以選擇任何一張圖片，並在此網站上將其畫風轉變成指定的藝術照\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://deepart.io/\" rel=\"nofollow\"\u003eDeepart.io\u003c/a\u003e 也提供類似服務\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"tensorflowjs\" class=\"anchor\" href=\"#tensorflowjs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://js.tensorflow.org/\" rel=\"nofollow\"\u003eTensorFlow.js\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTensorFlow.js 頁面有多個利用 JavaScript 實現的深度學習應用，如上圖中的\u003ca href=\"https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\" rel=\"nofollow\"\u003e人類姿勢估計 Human Pose Estimation\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e你可以在該應用裡頭打開自己的攝影機，看該應用能不能偵測到你與朋友的姿勢。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://poloclub.github.io/ganlab/\" rel=\"nofollow\"\u003eGAN Lab\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://talktotransformer.com/\" rel=\"nofollow\"\u003eTalk to Transformer\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://poloclub.github.io/ganlab/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/gan-lab.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://talktotransformer.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/talk_to_transformer.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"gan-lab\" class=\"anchor\" href=\"#gan-lab\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://poloclub.github.io/ganlab/\" rel=\"nofollow\"\u003eGAN Lab\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C\" rel=\"nofollow\"\u003e對抗生成網路（\u003cstrong\u003eG\u003c/strong\u003eenerative \u003cstrong\u003eA\u003c/strong\u003edversarial \u003cstrong\u003eN\u003c/strong\u003eetwork，簡稱GAN）\u003c/a\u003e是非監督式學習的一種方法，通過讓兩個神經網路相互博弈的方式進行學習。此網站以 \u003ca href=\"https://js.tensorflow.org/\" rel=\"nofollow\"\u003eTensorFlow.js\u003c/a\u003e 實作 GAN 中兩個神經網路的學習過程，幫助有興趣的你更直觀地理解神奇的 GAN 的運作方式\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"talk-to-transformer\" class=\"anchor\" href=\"#talk-to-transformer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://talktotransformer.com/\" rel=\"nofollow\"\u003eTalk to Transformer\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e展示了一個由 OpenAI 推出，名為 \u003ca href=\"https://openai.com/blog/better-language-models/\" rel=\"nofollow\"\u003eGPT-2 的無監督式語言模型\u003c/a\u003e。該模型以 Google 發表的神經網路架構 \u003ca href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\" rel=\"nofollow\"\u003eTransformer\u003c/a\u003e 為基底，在給定一段魔戒或是復仇者聯盟的文字內容，該模型可以自己生成唯妙唯俏的延伸劇情。你也可以嘗試 \u003ca href=\"https://gpt2.apps.allenai.org/?text=Joel%20is\" rel=\"nofollow\"\u003eAllenAI GPT-2 Explorer\u003c/a\u003e 來觀察 GPT-2 預測下個字的機率。\u003c/li\u003e\n\u003cli\u003e想要深入了解 Transformer 或 GPT-2，推薦閱讀：\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://leemeng.tw/neural-machine-translation-with-transformer-and-tensorflow2.html\" rel=\"nofollow\"\u003e淺談神經機器翻譯 \u0026amp; 用 Transformer 與 TensorFlow 2 英翻中\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html\" rel=\"nofollow\"\u003e直觀理解 GPT-2 語言模型並生成金庸武俠小說\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://jalammar.github.io/illustrated-gpt2/\" rel=\"nofollow\"\u003eThe Illustrated GPT-2 (Visualizing Transformer Language Models)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://www.nvidia.com/en-us/research/ai-playground/\" rel=\"nofollow\"\u003eNVIDIA AI PLAYGROUND\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://grover.allenai.org/\" rel=\"nofollow\"\u003eGrover\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://www.nvidia.com/en-us/research/ai-playground/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/nvidia-ai-playground.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://grover.allenai.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/grover.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"nvidia-ai-playground\" class=\"anchor\" href=\"#nvidia-ai-playground\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://www.nvidia.com/en-us/research/ai-playground/\" rel=\"nofollow\"\u003eNVIDIA AI PLAYGROUND\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e提供 \u003ca href=\"https://arxiv.org/abs/1903.07291\" rel=\"nofollow\"\u003eGauGAN\u003c/a\u003e 的線上展示，讓你可以利用簡單的筆觸來生成真實世界的風景圖片，也能上傳自己的圖片做風格轉換\u003c/li\u003e\n\u003cli\u003e提供 \u003ca href=\"https://arxiv.org/abs/1804.07723\" rel=\"nofollow\"\u003eImage Impainting\u003c/a\u003e 服務，讓使用者自由抹去部分圖片並讓 AI 自動生成被抹去的區塊\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"grover\" class=\"anchor\" href=\"#grover\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://grover.allenai.org/\" rel=\"nofollow\"\u003eGrover\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e一個偵測 / 生成神經假新聞（Neural Fake News）的研究，其網頁展示如何自動生成假新聞。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://waifulabs.com\" rel=\"nofollow\"\u003eWaifu Vending Machine\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://www.thiswaifudoesnotexist.net/\" rel=\"nofollow\"\u003eThis Waifu Does Not Exist\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://waifulabs.com\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/waifulabs.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://www.thiswaifudoesnotexist.net/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/thiswaifudoesnotexist.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"waifu-vending-machine\" class=\"anchor\" href=\"#waifu-vending-machine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://waifulabs.com\" rel=\"nofollow\"\u003eWaifu Vending Machine\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eWaifu 來自日文 ワイフ，指的是一些非常受到歡迎、且被不少玩家/觀眾視為妻子的動漫女性角色。\u003ca href=\"https://twitter.com/SizigiStudios\" rel=\"nofollow\"\u003eSizigi Studios\u003c/a\u003e 團隊利用 GAN 隨機初始 16 名虛擬動漫角色，讓使用者可以進一步依照喜愛來創造專屬於自己的 Waifu。\u003c/li\u003e\n\u003cli\u003eWaifu Vending Machine 產生的 Waifu 品質很高，使用者可以下載並分享自己創造的 Waifu，也可以選擇購買印製該 Waifu 的海報與抱枕。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"this-waifu-does-not-exist\" class=\"anchor\" href=\"#this-waifu-does-not-exist\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://www.thiswaifudoesnotexist.net/\" rel=\"nofollow\"\u003eThis Waifu Does Not Exist\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e以 Nvidia 的 \u003ca href=\"https://github.com/NVlabs/stylegan\" rel=\"nofollow\"\u003eStyleGAN\u003c/a\u003e 隨機生成的 Waifu（右圖左側）。作者 \u003ca href=\"https://www.gwern.net/\" rel=\"nofollow\"\u003eGwern\u003c/a\u003e 同時也使用\u003ca href=\"https://blog.openai.com/better-language-models/\" rel=\"nofollow\"\u003e開源的小型 GPT-2\u003c/a\u003e 隨機生成一段動漫劇情（右圖右側）。自釋出後已超越一百萬使用者拜訪該網站。\u003c/li\u003e\n\u003cli\u003e你也可以用大螢幕查看作者的另個相關網站：\u003ca href=\"https://www.obormot.net/demos/these-waifus-do-not-exist\" rel=\"nofollow\"\u003eThese Waifus Do Not Exist\u003c/a\u003e，用全畫面一次「觀賞」數十名隨機生成的 Waifus。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"http://www.deeplearning.ai/ai-notes/\" rel=\"nofollow\"\u003eAI Notes\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://anomagram.fastforwardlabs.com/#/\" rel=\"nofollow\"\u003eAnomagram\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"http://www.deeplearning.ai/ai-notes/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/deeplearning-ai-notes.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://anomagram.fastforwardlabs.com/#/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/playground/anomagram.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"ai-notes\" class=\"anchor\" href=\"#ai-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://www.deeplearning.ai/ai-notes/\" rel=\"nofollow\"\u003eAI Notes\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAI Notes 是 \u003ca href=\"#deep-learning-specialization--coursera\"\u003e吳恩達的 Deep Learning 專項課程\u003c/a\u003e的輔助教材，使用數學證明以及由 TensorFlow.js 建立的線上 demo 讓你可以直觀地學習\u003ca href=\"http://www.deeplearning.ai/ai-notes/initialization/\" rel=\"nofollow\"\u003e如何初始化神經網路權重\u003c/a\u003e及\u003ca href=\"http://www.deeplearning.ai/ai-notes/optimization/\" rel=\"nofollow\"\u003e如何最佳化模型權重\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e縮圖為 \u003ca href=\"http://www.deeplearning.ai/ai-notes/optimization/\" rel=\"nofollow\"\u003eParameter optimization in neural networks\u003c/a\u003e 單元中使用不同 Optimiziers 訓練模型的線上 demo\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"anomagram\" class=\"anchor\" href=\"#anomagram\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://anomagram.fastforwardlabs.com/#/\" rel=\"nofollow\"\u003eAnomagram\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAnomagram 是一個以 Tensorflow.js 實作，可以建立、訓練並測試能夠用來做異常檢測的 Autoencoder。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"線上課程\" class=\"anchor\" href=\"#%E7%B7%9A%E4%B8%8A%E8%AA%B2%E7%A8%8B\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cdiv\u003e線上課程\u003c/div\u003e\n\u003c/h2\u003e\n\u003cp\u003e看完\u003ca href=\"#playground\"\u003e遊玩空間\u003c/a\u003e的大量實際應用，相信你已經迫不及待地想要開始學習強大的深度學習技術了。\u003c/p\u003e\n\u003cp\u003e這節列舉了一些有用的線上課程以及學習教材，幫助你掌握深度學習的基本知識（沒有特別註明的話皆為免費存取）。\u003c/p\u003e\n\u003cp\u003e另外值得一提的是，大部分課程都要求一定程度的 \u003ca href=\"https://www.python.org/\" rel=\"nofollow\"\u003ePython\u003c/a\u003e 程式能力。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"http://speech.ee.ntu.edu.tw/%7Etlkagk/courses_ML19.html\" rel=\"nofollow\"\u003e李宏毅教授的機器學習 / 深度學習課程\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://www.coursera.org/specializations/deep-learning\" rel=\"nofollow\"\u003eDeep Learning Specialization @ Coursera\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/Hung-Yi-Lee-ml-courses.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://www.coursera.org/specializations/deep-learning\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/deep-learning-specification-coursera.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"李宏毅教授的機器學習--深度學習課程\" class=\"anchor\" href=\"#%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%95%99%E6%8E%88%E7%9A%84%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92--%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E8%AA%B2%E7%A8%8B\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://speech.ee.ntu.edu.tw/%7Etlkagk/courses_ML20.html\" rel=\"nofollow\"\u003e李宏毅教授的機器學習 / 深度學習課程\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e大概是全世界最好、最完整的深度學習\u003cb\u003e中文\u003c/b\u003e學習資源，且作業皆提供 Colab 筆記本範例。\u003c/li\u003e\n\u003cli\u003e影片內容涵蓋基本理論（約 10 小時觀看時間）一直到進階的\u003ca href=\"https://zh.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C\" rel=\"nofollow\"\u003e生成對抗網路 GAN\u003c/a\u003e 以及\u003ca href=\"https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\" rel=\"nofollow\"\u003e強化學習 RL\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e想學語音辨識或是自然語言處理則可參考教授的\u003ca href=\"http://speech.ee.ntu.edu.tw/%7Etlkagk/courses_DLHLP20.html\" rel=\"nofollow\"\u003e用深度學習處理人類語言\u003c/a\u003e。\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/datawhalechina/leeml-notes\" rel=\"nofollow\"\u003e李宏毅机器学习笔记(LeeML-Notes，簡體)\u003c/a\u003e 則將教授上課的影片內容轉為筆記，方便瀏覽課程內容。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"deep-learning-specialization--coursera\" class=\"anchor\" href=\"#deep-learning-specialization--coursera\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://www.coursera.org/specializations/deep-learning\" rel=\"nofollow\"\u003eDeep Learning Specialization @ Coursera\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e原 Google Brain 的\u003ca href=\"https://zh.wikipedia.org/wiki/%E5%90%B4%E6%81%A9%E8%BE%BE\" rel=\"nofollow\"\u003e吳恩達\u003c/a\u003e教授開授的整個深度學習專項課程共分五堂課，從\u003ca href=\"https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning\" rel=\"nofollow\"\u003e神經網路的基礎\u003c/a\u003e到能夠進行機器翻譯、語音辨識的\u003ca href=\"https://www.coursera.org/learn/nlp-sequence-models\" rel=\"nofollow\"\u003e序列模型\u003c/a\u003e，每堂課預計 1 個月完成，收費採訂閱制\u003c/li\u003e\n\u003cli\u003e程式作業會交互使用 \u003ca href=\"http://www.numpy.org/\" rel=\"nofollow\"\u003eNumpy\u003c/a\u003e、\u003ca href=\"https://keras.io/\" rel=\"nofollow\"\u003eKeras\u003c/a\u003e 以及 \u003ca href=\"https://www.tensorflow.org/\" rel=\"nofollow\"\u003eTensorFlow\u003c/a\u003e 來實作深度學習模型\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://course.fast.ai/index.html\" rel=\"nofollow\"\u003ePractical Deep Learning For Coders @ fast.ai\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://www.kaggle.com/learn/deep-learning\" rel=\"nofollow\"\u003eDeep Learning @ Kaggle Learn\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://course.fast.ai/index.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/fast-ai.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://www.kaggle.com/learn/deep-learning\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/kaggle-learn-dl.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"practical-deep-learning-for-coders--fastai\" class=\"anchor\" href=\"#practical-deep-learning-for-coders--fastai\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://course.fast.ai/index.html\" rel=\"nofollow\"\u003ePractical Deep Learning For Coders @ fast.ai\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e7 週課程，一週約需安排 10 小時上課。該課程由\u003ca href=\"https://www.kaggle.com/jhoward\" rel=\"nofollow\"\u003e傑里米·霍華德\u003c/a\u003e來講解深度學習，其在知名數據建模和數據分析競賽平台 \u003ca href=\"https://www.kaggle.com/\" rel=\"nofollow\"\u003eKaggle\u003c/a\u003e 維持兩年的世界第一\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"deep-learning--kaggle-learn\" class=\"anchor\" href=\"#deep-learning--kaggle-learn\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://www.kaggle.com/learn/deep-learning\" rel=\"nofollow\"\u003eDeep Learning @ Kaggle Learn\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e14 堂課程，主要使用 TensorFlow 實作深度學習模型\u003c/li\u003e\n\u003cli\u003e內容主要專注在\u003ca href=\"https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89\" rel=\"nofollow\"\u003e電腦視覺（Computer Vision）\u003c/a\u003e以及如何應用\u003ca href=\"https://en.wikipedia.org/wiki/Transfer_learning\" rel=\"nofollow\"\u003e遷移學習（Transfer Learning）\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://www.elementsofai.com/\" rel=\"nofollow\"\u003eElements of Artificial Intelligence\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://deeplearning.mit.edu/\" rel=\"nofollow\"\u003eMIT Deep Learning\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://www.elementsofai.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/elementsofai.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://selfdrivingcars.mit.edu/deeptraffic\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/mlt-deep-learning.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"elements-of-artificial-intelligence\" class=\"anchor\" href=\"#elements-of-artificial-intelligence\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://www.elementsofai.com/\" rel=\"nofollow\"\u003eElements of Artificial Intelligence\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e芬蘭最高學府\u003ca href=\"https://zh.wikipedia.org/wiki/%E8%B5%AB%E5%B0%94%E8%BE%9B%E5%9F%BA%E5%A4%A7%E5%AD%A6\" rel=\"nofollow\"\u003e赫爾辛基大學\u003c/a\u003e推出的 AI 課程。此課程目的在於讓所有人都能了解 AI，不需要任何程式經驗。這堂課非常適合完全沒有接觸過深度學習或是相關領域的人\u003c/li\u003e\n\u003cli\u003e課程分 6 個部分，包含「何謂 AI ？」、「真實世界的 AI」、「機器學習」以及「神經網路」等章節\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"mit-deep-learning\" class=\"anchor\" href=\"#mit-deep-learning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://deeplearning.mit.edu/\" rel=\"nofollow\"\u003eMIT Deep Learning\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e麻省理工學院推出的深度學習課程，內容包含深度學習基礎、深度強化學習以及自動駕駛相關知識。\u003ca href=\"https://github.com/lexfridman/mit-deep-learning\" rel=\"nofollow\"\u003eGithub Repo\u003c/a\u003e 包含了多個教學筆記本，值得參考。\u003c/li\u003e\n\u003cli\u003e上圖是 \u003ca href=\"https://selfdrivingcars.mit.edu/deeptraffic/\" rel=\"nofollow\"\u003eDeepTraffic\u003c/a\u003e，由 MIT 的研究科學家 \u003ca href=\"https://lexfridman.com/\" rel=\"nofollow\"\u003eLex Fridman\u003c/a\u003e 推出的一個深度強化學習競賽。此競賽目標是建立一個可以在高速公路上駕駛汽車的神經網路。你可以在\u003ca href=\"https://selfdrivingcars.mit.edu/deeptraffic/\" rel=\"nofollow\"\u003e這裡\u003c/a\u003e看到線上 Demo 以及詳細說明。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"http://introtodeeplearning.com\" rel=\"nofollow\"\u003e6.S191: Introduction to Deep Learning\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://www.coursera.org/learn/ai-for-everyone\" rel=\"nofollow\"\u003eAI For Everyone\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"http://introtodeeplearning.com\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/intro-to-deeplearning-mit.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://www.coursera.org/learn/ai-for-everyone\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/ai-for-everyone.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"mit-6s191-introduction-to-deep-learning\" class=\"anchor\" href=\"#mit-6s191-introduction-to-deep-learning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://introtodeeplearning.com/\" rel=\"nofollow\"\u003eMIT 6.S191 Introduction to Deep Learning\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e麻省理工學院推出的另一堂基礎深度學習課程，介紹深度學習以及其應用。內容涵蓋機器翻譯、圖像辨識以及更多其他應用。此課程使用 Python 以及 TensorFlow 來實作作業，並預期學生具備基礎的微積分（梯度下降、鏈鎖律）以及線性代數（矩陣相乘）。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"ai-for-everyone\" class=\"anchor\" href=\"#ai-for-everyone\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://www.coursera.org/learn/ai-for-everyone\" rel=\"nofollow\"\u003eAI For Everyone\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCoursera 課程。\u003ca href=\"https://zh.wikipedia.org/wiki/%E5%90%B4%E6%81%A9%E8%BE%BE\" rel=\"nofollow\"\u003e吳恩達\u003c/a\u003e教授在這堂簡短的課程裡頭，針對非技術人士以及企業經理人說明何謂 AI、如何建立 AI 專案以及闡述 AI 與社會的關係。此課程十分適合沒有技術背景的讀者。\u003ca href=\"https://leemeng.tw/10-key-takeaways-from-ai-for-everyone-course.html\" rel=\"nofollow\"\u003e從 AI For Everyone 學到的 10 個重要 AI 概念\u003c/a\u003e則是我個人上完課後整理的心得分享。\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"http://web.stanford.edu/class/cs224n/\" rel=\"nofollow\"\u003eCS224n: Natural Language Processing with Deep Learning\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"http://cs231n.stanford.edu/\" rel=\"nofollow\"\u003eCS231n: Convolutional Neural Networks for Visual Recognition\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/leemengtaiwan/deep-learning-resources/blob/master/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/cs224n.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/leemengtaiwan/deep-learning-resources/blob/master/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/courses/cs231n.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"cs224n-natural-language-processing-with-deep-learning\" class=\"anchor\" href=\"#cs224n-natural-language-processing-with-deep-learning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://web.stanford.edu/class/cs224n/\" rel=\"nofollow\"\u003eCS224n: Natural Language Processing with Deep Learning\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e由\u003ca href=\"http://technews.tw/2018/11/21/stanford-ai-lab-christopher-manning/\" rel=\"nofollow\"\u003e史丹佛 AI 實驗室的 Christopher Manning 教授\u003c/a\u003e從語言學、計算機科學的角度講述自然語言處理的所有必要知識，是想要打好 NLP 基礎的人不可不學的一堂課。課程約有 20 部影片，每部約長 1.5 小時。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"cs231n-convolutional-neural-networks-for-visual-recognition\" class=\"anchor\" href=\"#cs231n-convolutional-neural-networks-for-visual-recognition\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://cs231n.stanford.edu/\" rel=\"nofollow\"\u003eCS231n: Convolutional Neural Networks for Visual Recognition\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e由\u003ca href=\"http://vision.stanford.edu/index.html\" rel=\"nofollow\"\u003e史丹佛 Vision Lab 的李飛飛（Fei-Fei Li）教授\u003c/a\u003e等人以\u003ca href=\"http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture02.pdf\" rel=\"nofollow\"\u003e圖像分類\u003c/a\u003e任務為軸心，講述卷積神經網路以及所有電腦視覺的相關基礎知識。這是想要學會使用（卷積）神經網路來處理圖像數據的人不可不學的一堂課。\u003ca href=\"https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\" rel=\"nofollow\"\u003eYoutube 上有 16 部 2017 年的課程錄影\u003c/a\u003e，每部約長 1 小時。\u003c/li\u003e\n\u003cli\u003e課程中也包含了不少線上展示，如\u003ca href=\"http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/\" rel=\"nofollow\"\u003e線性分類器的 loss 視覺化\u003c/a\u003e、\u003ca href=\"http://vision.stanford.edu/teaching/cs231n-demos/knn/\" rel=\"nofollow\"\u003ekNN demo\u003c/a\u003e 以及圖像分類的 \u003ca href=\"http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\" rel=\"nofollow\"\u003eCIFAR-10 demo\u003c/a\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"實用工具\" class=\"anchor\" href=\"#%E5%AF%A6%E7%94%A8%E5%B7%A5%E5%85%B7\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cdiv\u003e實用工具\u003c/div\u003e\n\u003c/h2\u003e\n\u003cp\u003e這節列出一些在你的深度學習路上可以幫得上些忙的工具。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://colab.research.google.com/notebooks/welcome.ipynb\" rel=\"nofollow\"\u003eColaboratory\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://www.tensorflow.org/guide/summaries_and_tensorboard\" rel=\"nofollow\"\u003eTensorBoard\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://colab.research.google.com/notebooks/welcome.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/colab.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://www.tensorflow.org/guide/summaries_and_tensorboard\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/tensorboard.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"colaboratory\" class=\"anchor\" href=\"#colaboratory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://colab.research.google.com/notebooks/welcome.ipynb\" rel=\"nofollow\"\u003eColaboratory\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e由 Google 提供的雲端 \u003ca href=\"https://jupyter.org/\" rel=\"nofollow\"\u003eJupyter\u003c/a\u003e 筆記本環境，讓你只要用瀏覽器就能馬上開始訓練深度學習模型。你甚至還可以使用一個免費的 \u003ca href=\"https://www.nvidia.com/en-gb/data-center/tesla-k80/\" rel=\"nofollow\"\u003eTesla K80\u003c/a\u003e GPU 或 \u003ca href=\"https://colab.research.google.com/notebooks/tpu.ipynb\" rel=\"nofollow\"\u003eTPU\u003c/a\u003e 來加速訓練自己的模型\u003c/li\u003e\n\u003cli\u003e該計算環境也能與自己的 \u003ca href=\"https://colab.research.google.com/notebooks/io.ipynb\" rel=\"nofollow\"\u003eGoogle Drive\u003c/a\u003e 做連結，讓運算雲端化的同時將筆記本 / 模型結果都同步到自己的筆電上\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"tensorboard\" class=\"anchor\" href=\"#tensorboard\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://www.tensorflow.org/guide/summaries_and_tensorboard\" rel=\"nofollow\"\u003eTensorBoard\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTensorBoard 是一個視覺化工具，方便我們了解、除錯並最佳化自己訓練的深度學習模型\u003c/li\u003e\n\u003cli\u003e除了 TensorFlow 以外，其他基於 Python 的機器學習框架大多也可以透過 \u003ca href=\"https://github.com/lanpa/tensorboardX\" rel=\"nofollow\"\u003etensorboardX\u003c/a\u003e 來使用 TensorBoard\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://projector.tensorflow.org/\" rel=\"nofollow\"\u003eEmbedding Projector\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://github.com/tensorflow/lucid\" rel=\"nofollow\"\u003eLucid\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://projector.tensorflow.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/embedding-projector.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/tensorflow/lucid\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/lucid.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"embedding-projector\" class=\"anchor\" href=\"#embedding-projector\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://projector.tensorflow.org/\" rel=\"nofollow\"\u003eEmbedding Projector\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e我們時常需要將圖片、文字轉成\u003ca href=\"https://en.wikipedia.org/wiki/Tensor\" rel=\"nofollow\"\u003e高維數字向量 Embedding\u003c/a\u003e 以供神經網路處理，而 Projector 能將此高維向量投影到 2、3 維空間上方便我們理解這些數據\u003c/li\u003e\n\u003cli\u003eProjector 網站讓你在線上探索幾個常見的資料集，但事實上你也可以\u003ca href=\"https://www.tensorflow.org/guide/embedding\" rel=\"nofollow\"\u003e利用 Tensorboard 來視覺化自己的數據\u003c/a\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"lucid\" class=\"anchor\" href=\"#lucid\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/tensorflow/lucid\" rel=\"nofollow\"\u003eLucid\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eLucid 是一個嘗試讓神經網路變得更容易解釋的開源專案，裡頭包含了很多視覺化神經網路的筆記本\u003c/li\u003e\n\u003cli\u003e你可以直接在 Colab 上執行這些筆記本並了解如何視覺化神經網路\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://paperswithcode.com/\" rel=\"nofollow\"\u003ePapers with Code\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://pair-code.github.io/what-if-tool/\" rel=\"nofollow\"\u003eWhat-If Tool\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://paperswithcode.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/papers-with-code.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://pair-code.github.io/what-if-tool/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/what-if-tool.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"papers-with-code\" class=\"anchor\" href=\"#papers-with-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://paperswithcode.com/\" rel=\"nofollow\"\u003ePapers with Code\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e將機器學習的學術論文、程式碼實作以及 SOTA 的評價排行榜全部整理匯總在一起的網站，非常適合想要持續追蹤學術及業界最新研究趨勢的人\u003c/li\u003e\n\u003cli\u003e在這邊可以瀏覽包含電腦視覺、自然語言處理等各大領域在不同任務上表現最好的論文、實作以及資料集\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"what-if-tool\" class=\"anchor\" href=\"#what-if-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://pair-code.github.io/what-if-tool/\" rel=\"nofollow\"\u003eWhat-If Tool\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e一個與 \u003ca href=\"#tensorboard\"\u003eTensorBoard\u003c/a\u003e 以及 Jupyter Notebook 整合的探索工具，讓使用者不需寫程式碼就能輕鬆觀察機器學習模型的內部運作以及嘗試各種 What-if 問題（如果 ~ 會怎麼樣？）\u003c/li\u003e\n\u003cli\u003e基本上就是用來觀察\u003cstrong\u003e已訓練\u003c/strong\u003e的模型在測試資料集上的表現。利用此工具，使用者可以了解（不僅限於）以下的問題：模型在各類別數據上的表現有無差距？模型是否存在偏見？應該如何調整 Native / Positive False 的比例？\u003c/li\u003e\n\u003cli\u003e此工具的一大亮點在於讓非專業領域人士也能探索、理解 ML 模型表現。且只要給定模型與資料集, 就不需要每次為了 What-if 問題就寫用過即丟的程式碼\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://github.com/jessevig/bertviz\" rel=\"nofollow\"\u003eBertViz\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://github.com/dair-ai/ml-visuals\" rel=\"nofollow\"\u003eML Visuals\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/jessevig/bertviz\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/bertviz.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/dair-ai/ml-visuals\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/ml-visuals.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"bertviz\" class=\"anchor\" href=\"#bertviz\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/jessevig/bertviz\" rel=\"nofollow\"\u003eBertViz\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eBertViz 是一個視覺化自注意力機制的工具，可以用來理解如 \u003ca href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\"\u003eBERT\u003c/a\u003e、\u003ca href=\"https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\" rel=\"nofollow\"\u003eGPT-2\u003c/a\u003e 及 \u003ca href=\"https://arxiv.org/abs/1907.11692\" rel=\"nofollow\"\u003eRoBERTa\u003c/a\u003e 等知名 NLP 模型的內部運作\u003c/li\u003e\n\u003cli\u003e以下則是幾篇透過 BertViz 來直觀解說 BERT 與 GPT-2 的文章\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html\" rel=\"nofollow\"\u003e進擊的 BERT：NLP 界的巨人之力與遷移學習\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html\" rel=\"nofollow\"\u003e直觀理解 GPT-2 語言模型並生成金庸武俠小說\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"ml-visuals\" class=\"anchor\" href=\"#ml-visuals\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/dair-ai/ml-visuals\" rel=\"nofollow\"\u003eML Visuals\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eML Visuals 是一個社群開源項目，提供超過 100 個常見的機器學習概念 / 深度學習架構圖，可讓任何人在學術論文或是文章直接使用這些圖表。\u003c/li\u003e\n\u003cli\u003e所有圖表都可以直接從 \u003ca href=\"https://docs.google.com/presentation/d/11mR1nkIR9fbHegFkcFq8z9oDQ5sjv8E3JJp1LfLGKuk/edit?usp=sharing\" rel=\"nofollow\"\u003eGoogle slide\u003c/a\u003e 上觀看並使用。建議前往 \u003ca href=\"https://github.com/dair-ai/ml-visuals\" rel=\"nofollow\"\u003eGithub repo\u003c/a\u003e 查看最新版本。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"其他教材\" class=\"anchor\" href=\"#%E5%85%B6%E4%BB%96%E6%95%99%E6%9D%90\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cdiv\u003e其他教材\u003c/div\u003e\n\u003c/h2\u003e\n\u003cp\u003e除了\u003ca href=\"#courses\"\u003e線上課程\u003c/a\u003e以外，網路上還有無數的學習資源。\u003c/p\u003e\n\u003cp\u003e這邊列出一些推薦的深度學習教材，大多數皆以數據科學家常用的 \u003ca href=\"https://jupyter.org/\" rel=\"nofollow\"\u003eJupyter\u003c/a\u003e 筆記本的方式呈現。\u003c/p\u003e\n\u003cp\u003e你可以將感興趣的筆記本導入\u003ca href=\"#tools\"\u003e實用工具\u003c/a\u003e裡提到的 \u003ca href=\"https://colab.research.google.com/notebooks/welcome.ipynb\" rel=\"nofollow\"\u003eColaboratory（Colab）\u003c/a\u003e，馬上開始學習。\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://research.google.com/seedbank/\" rel=\"nofollow\"\u003eSeedbank\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://github.com/fchollet/deep-learning-with-python-notebooks\" rel=\"nofollow\"\u003eDeep Learning with Python\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://research.google.com/seedbank/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/seedbank.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/fchollet/deep-learning-with-python-notebooks\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/fchollet-deep-learning-with-python.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"seedbank\" class=\"anchor\" href=\"#seedbank\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://research.google.com/seedbank/\" rel=\"nofollow\"\u003eSeedbank\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e讓你可以一覽 Colab 上超過 100 個跟機器學習相關的筆記本，並以此為基礎建立各種深度學習應用\u003c/li\u003e\n\u003cli\u003e熱門筆記本包含\u003ca href=\"https://research.google.com/seedbank/seed/5695159920492544\" rel=\"nofollow\"\u003e神經機器翻譯\u003c/a\u003e、\u003ca href=\"https://research.google.com/seedbank/seed/5681034041491456\" rel=\"nofollow\"\u003e音樂生成\u003c/a\u003e以及 \u003ca href=\"https://research.google.com/seedbank/seed/5631986051842048\" rel=\"nofollow\"\u003eDeepDream\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e因為是 Google 服務，筆記本大多使用 TensorFlow 與 Keras 來實現模型\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"deep-learning-with-python\" class=\"anchor\" href=\"#deep-learning-with-python\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/fchollet/deep-learning-with-python-notebooks\" rel=\"nofollow\"\u003eDeep Learning with Python\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://keras.io/\" rel=\"nofollow\"\u003eKeras\u003c/a\u003e 作者 \u003ca href=\"https://ai.google/research/people/105096\" rel=\"nofollow\"\u003eFrançois Chollet\u003c/a\u003e 在 \u003ca href=\"https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438\" rel=\"nofollow\"\u003eDeep Learning with Python\u003c/a\u003e 一書中用到的所有筆記本。每個筆記本裡頭都清楚地介紹該如何使用 Keras 來實現各種深度學習模型，十分適合第一次使用 Python 實現深度學習的讀者\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#top\" rel=\"nofollow\"\u003e進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南\u003c/a\u003e一文的 Keras 程式碼大多基於此\u003c/li\u003e\n\u003cli\u003e繁體中文的翻譯書籍則為 \u003ca href=\"https://www.tenlong.com.tw/products/9789863125501?list_name=i-r-zh_tw\" rel=\"nofollow\"\u003eDeep learning 深度學習必讀 - Keras 大神帶你用 Python 實作\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eKeras 在 TensorFlow 2.0 中\u003ca href=\"https://medium.com/tensorflow/standardizing-on-keras-guidance-on-high-level-apis-in-tensorflow-2-0-bad2b04c819a\" rel=\"nofollow\"\u003e為其最重要的高層次 API\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\" rel=\"nofollow\"\u003eStanford CS230 Cheatsheets\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://github.com/madewithml/practicalAI\" rel=\"nofollow\"\u003epracticalAI\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/cs230-deep-learning-cheatsheet.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/GokuMohandas/practicalAI\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/practical-ai-pytorch.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"stanford-cs230-cheatsheets\" class=\"anchor\" href=\"#stanford-cs230-cheatsheets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\" rel=\"nofollow\"\u003eStanford CS230 Cheatsheets\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e史丹佛大學的\u003ca href=\"http://cs230.stanford.edu/\" rel=\"nofollow\"\u003e深度學習課程 CS230\u003c/a\u003e 釋出的深度學習小抄總結了目前最新的\u003ca href=\"https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\" rel=\"nofollow\"\u003e卷積神經網路\u003c/a\u003e及\u003ca href=\"https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\" rel=\"nofollow\"\u003e循環神經網路\u003c/a\u003e知識，還包含了\u003ca href=\"https://stanford.edu/%7Eshervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks\" rel=\"nofollow\"\u003e訓練深度學習時需要使用到的技巧\u003c/a\u003e，十分強大\u003c/li\u003e\n\u003cli\u003e此小抄最適合已經熟悉基礎知識的同學隨時複習運用。你也可以從他們的 \u003ca href=\"https://github.com/afshinea/stanford-cs-230-deep-learning\" rel=\"nofollow\"\u003eGithub Repo\u003c/a\u003e 下載包含上述所有內容的\u003ca href=\"https://github.com/afshinea/stanford-cs-230-deep-learning/blob/master/en/super-cheatsheet-deep-learning.pdf\" rel=\"nofollow\"\u003e超級 VIP 小抄\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e除了深度學習以外，你也可以查看 \u003ca href=\"https://stanford.edu/%7Eshervine/teaching/cs-229.html\" rel=\"nofollow\"\u003eCS229 機器學習課程的小抄\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"practicalai\" class=\"anchor\" href=\"#practicalai\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/madewithml/practicalAI\" rel=\"nofollow\"\u003epracticalAI\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e在 Github 上超過 1 萬星的 Repo。除了深度學習，也有介紹 \u003ca href=\"https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/basic_ml/01_Python.ipynb\" rel=\"nofollow\"\u003ePython 基礎\u003c/a\u003e及 \u003ca href=\"https://colab.research.google.com/github/practicalAI/practicalAI/blob/master/notebooks/basic_ml/03_Pandas.ipynb\" rel=\"nofollow\"\u003ePandas\u003c/a\u003e 的使用方式\u003c/li\u003e\n\u003cli\u003e使用 \u003ca href=\"https://pytorch.org/\" rel=\"nofollow\"\u003ePyTorch\u003c/a\u003e 框架來實現深度學習模型，且所有內容都是 Jupyter 筆記本，可以讓你在 Colab 或本地端執行\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"http://demo.allennlp.org/\" rel=\"nofollow\"\u003eAllenNLP Demo\u003c/a\u003e\u003c/th\u003e\n\u003cth align=\"center\"\u003e\u003ca href=\"https://github.com/ageron/handson-ml2\" rel=\"nofollow\"\u003eHands-on Machine Learning 2\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"http://demo.allennlp.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tools/allennlp-demo.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/ageron/handson-ml2\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/leemengtaiwan/deep-learning-resources/raw/master/images/tutorials/handson-ml2.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"allennlp-demo\" class=\"anchor\" href=\"#allennlp-demo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://demo.allennlp.org/\" rel=\"nofollow\"\u003eAllenNLP Demo\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e清楚地展示了如\u003ca href=\"https://demo.allennlp.org/machine-comprehension\" rel=\"nofollow\"\u003e機器理解\u003c/a\u003e、\u003ca href=\"https://demo.allennlp.org/named-entity-recognition\" rel=\"nofollow\"\u003e命名實體識別\u003c/a\u003e等多個自然語言處理任務的情境。每個任務的情境包含了任務所需要的輸入、SOTA 模型的預測結果以及模型內部的注意力機制，對理解一個 NLP 任務的實際應用情境有很大幫助\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://allennlp.org/\" rel=\"nofollow\"\u003eAllenNLP\u003c/a\u003e 是一個由 \u003ca href=\"https://allenai.org/\" rel=\"nofollow\"\u003eAI2\u003c/a\u003e 以 \u003ca href=\"https://pytorch.org/\" rel=\"nofollow\"\u003ePyTorch\u003c/a\u003e 實現的自然語言處理函式庫\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"hands-on-machine-learning-2\" class=\"anchor\" href=\"#hands-on-machine-learning-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/ageron/handson-ml2\" rel=\"nofollow\"\u003eHands-on Machine Learning 2\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e前 YouTube 影片分類 PM \u003ca href=\"https://twitter.com/aureliengeron\" rel=\"nofollow\"\u003eAurélien Geron\u003c/a\u003e 教你如何透過 Scikit-Learn、Keras 以及 TensorFlow 2 來進行機器學習以及深度學習任務與應用的筆記本彙整。\u003c/li\u003e\n\u003cli\u003e第二版專注在 TensorFlow 2，其 Github repo 已有超過 6 千顆星，\u003ca href=\"https://github.com/ageron/handson-ml\" rel=\"nofollow\"\u003e第一版\u003c/a\u003e則有高達 2 萬星。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"優質文章\" class=\"anchor\" href=\"#%E5%84%AA%E8%B3%AA%E6%96%87%E7%AB%A0\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cdiv\u003e優質文章\u003c/div\u003e\n\u003c/h2\u003e\n\u003cp\u003e這邊列舉了一些幫助我釐清重要概念的部落格以及網站，希望能加速你探索這個深度學習世界。\u003c/p\u003e\n\u003cp\u003e只要 Google 一下就能發現這些部落格裡頭很多文章都有中文翻譯。但為了尊重原作者，在這邊都列出原文連結。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://distill.pub/about/\" rel=\"nofollow\"\u003eDistill\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e用非常高水準且互動的方式來說明複雜的深度學習概念。\u003ca href=\"http://www.iro.umontreal.ca/%7Ebengioy/yoshua_en/index.html\" rel=\"nofollow\"\u003eYoshua Bengio\u003c/a\u003e、\u003ca href=\"http://www.iangoodfellow.com/\" rel=\"nofollow\"\u003eIan Goodfellow\u003c/a\u003e 及 \u003ca href=\"http://cs.stanford.edu/people/karpathy/\" rel=\"nofollow\"\u003eAndrej Karpathy\u003c/a\u003e 等知名人士皆參與其中\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.r2d3.us/%E5%9C%96%E8%A7%A3%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%AC%AC%E4%B8%80%E7%AB%A0/\" rel=\"nofollow\"\u003eR2D3: 圖解機器學習\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e利用非常直覺易懂的視覺化來說明機器學習，連結為中文版\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://colah.github.io/\" rel=\"nofollow\"\u003eChristopher Olah's blog\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e詳細解釋不少深度學習概念。作者在\u003ca href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"nofollow\"\u003e這篇\u003c/a\u003e就詳細地解釋了\u003ca href=\"https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%A8%98%E6%86%B6%E5%8A%9B%E5%A5%BD%E7%9A%84-LSTM-%E7%B4%B0%E8%83%9E\" rel=\"nofollow\"\u003e長短期記憶 LSTM\u003c/a\u003e 的概念與變形；在\u003ca href=\"http://colah.github.io/posts/2014-07-Understanding-Convolutions/\" rel=\"nofollow\"\u003e這篇\u003c/a\u003e則解釋何為 CNN 的卷積運算\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://jalammar.github.io/\" rel=\"nofollow\"\u003eJay Alammar's blog\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e以清楚易懂的視覺化解釋深度學習概念。\u003ca href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\" rel=\"nofollow\"\u003e這篇\u003c/a\u003e用大量易懂的動畫說明\u003ca href=\"https://en.wikipedia.org/wiki/Neural_machine_translation\" rel=\"nofollow\"\u003e神經機器翻譯\u003c/a\u003e，而在\u003ca href=\"https://jalammar.github.io/illustrated-bert/\" rel=\"nofollow\"\u003e這篇\u003c/a\u003e則介紹如何利用如 \u003ca href=\"https://allennlp.org/elmo\" rel=\"nofollow\"\u003eELMo\u003c/a\u003e、\u003ca href=\"https://github.com/google-research/bert\" rel=\"nofollow\"\u003eBERT\u003c/a\u003e 等預先訓練過的強大模型在自然語言處理進行\u003ca href=\"https://en.wikipedia.org/wiki/Transfer_learning\" rel=\"nofollow\"\u003e遷移學習\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://karpathy.github.io/\" rel=\"nofollow\"\u003eAndrej Karpathy's blog\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e現為 Tesla AI 負責人的 \u003ca href=\"https://twitter.com/karpathy\" rel=\"nofollow\"\u003eAndrej Karpathy\u003c/a\u003e 在\u003ca href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" rel=\"nofollow\"\u003e這篇\u003c/a\u003e明確說明何謂循環神經網路 RNN。文中提供不少應用實例及視覺化來幫助我們理解 RNN 模型究竟學到了什麼，是學習 RNN 的朋友幾乎一定會碰到的一篇文章\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"經典論文\" class=\"anchor\" href=\"#%E7%B6%93%E5%85%B8%E8%AB%96%E6%96%87\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cdiv\u003e經典論文\u003c/div\u003e\n\u003c/h2\u003e\n\u003cp\u003e這邊依發表時間列出深度學習領域的經典 / 重要論文。\u003c/p\u003e\n\u003cp\u003e為了幫助你快速掌握論文內容以及歷年的研究趨勢，每篇論文下會有非常簡短的介紹（WIP）。\u003c/p\u003e\n\u003cp\u003e但我們推薦有興趣的人自行閱讀論文以深入了解。\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"自然語言處理-natural-language-processing-nlp\" class=\"anchor\" href=\"#%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-natural-language-processing-nlp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e自然語言處理 Natural Language Processing (NLP)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" rel=\"nofollow\"\u003e2003/02 A Neural Probabilistic Language Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1301.3781\" rel=\"nofollow\"\u003e2013/01 Efficient Estimation of Word Representations in Vector Space\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1308.0850\" rel=\"nofollow\"\u003e2013/08 Generating Sequences With Recurrent Neural Networks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1409.0473\" rel=\"nofollow\"\u003e2014/09 Neural Machine Translation by Jointly Learning to Align and Translate\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1508.04025\" rel=\"nofollow\"\u003e2015/08 Effective Approaches to Attention-based Neural Machine Translation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://arxiv.org/abs/1511.01432\" rel=\"nofollow\"\u003e2015/12 Semi-supervised Sequence Learning\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e推出一套無監督式的預訓練方法。使用無標籤數據訓練後的 RNN 模型在之後的監督式任務表現更好\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://arxiv.org/abs/1706.03762\" rel=\"nofollow\"\u003e2017/06 Attention Is All You Need\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle 推出新的神經網路架構 \u003ca href=\"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\" rel=\"nofollow\"\u003eTransformer\u003c/a\u003e。這個基於自注意力機制的架構特別適合語言理解任務\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1706.05137\" rel=\"nofollow\"\u003e2017/06 One Model To Learn Them All\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://arxiv.org/abs/1708.00107\" rel=\"nofollow\"\u003e2017/08 Learned in Translation: Contextualized Word Vectors\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e監督式預訓練。透過 BiLSTM 與 Encoder-Decoder 架構預先訓練機器翻譯任務並將訓練後的 Encoder 拿來做特徵擷取。將 Encoder 的輸出作為語境向量（Context Vectors, CoVe）處理下游任務\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1801.06146\" rel=\"nofollow\"\u003e2018/01 Universal Language Model Fine-tuning for Text Classification\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://arxiv.org/abs/1802.05365\" rel=\"nofollow\"\u003e2018/02 Deep contextualized word representations\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://allennlp.org/elmo\" rel=\"nofollow\"\u003eELMo 詞向量\u003c/a\u003e，利用兩獨立訓練的 LSTM 獲取雙向訊息\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\" rel=\"nofollow\"\u003e2018/06 Improving Language Understanding by Generative Pre-Training\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://blog.openai.com/language-unsupervised/\" rel=\"nofollow\"\u003eOpenAI\u003c/a\u003e 利用無監督式預訓練以及 Transformer 架構訓練出來的模型表現在多個 NLP 任務表現良好。約使用 8 億詞彙量的資料集\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://arxiv.org/abs/1810.04805\" rel=\"nofollow\"\u003e2018/10 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eGoogle 暴力美學。利用深層 Transformer 架構、2 個精心設計的預訓練任務以及約 33 億詞彙量的資料集訓練後，得到表現卓越的語言代表模型，打破 11 項 NLP 任務紀錄\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://arxiv.org/abs/1905.02450\" rel=\"nofollow\"\u003e2019/05 MASS: Masked Sequence to Sequence Pre-training for Language Generation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eMicrosoft 利用 Encoder-Decoder 架構以及連續遮罩（consecutive mask）將 BERT 推廣到自然語言生成（NLG）類型任務\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://arxiv.org/abs/1905.03197\" rel=\"nofollow\"\u003e2019/05 Unified Language Model Pre-training for Natural Language Understanding and Generation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e預訓練階段利用不同遮罩控制 context，同時訓練雙向 LM、單向 LM 以及 Seq2Seq LM。其產生的預訓練模型可以處理 NLU 以及 NLG 任務，並在不加入外部數據的情況下打敗 BERT 在 GLUE 的紀錄\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"電腦視覺-computer-vision-cv\" class=\"anchor\" href=\"#%E9%9B%BB%E8%85%A6%E8%A6%96%E8%A6%BA-computer-vision-cv\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e電腦視覺 Computer Vision (CV)\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"類神經網路架構-neural-network-architecture\" class=\"anchor\" href=\"#%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E6%9E%B6%E6%A7%8B-neural-network-architecture\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e類神經網路架構 Neural Network Architecture\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" rel=\"nofollow\"\u003e1998/01 Gradient-Based Learning Applied to Document Recognition (LeNet-5)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\" rel=\"nofollow\"\u003e2012/12 ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.cs.toronto.edu/%7Eranzato/publications/taigman_cvpr14.pdf\" rel=\"nofollow\"\u003e2014/06 DeepFace: Closing the Gap to Human-Level Performance in Face Verification (DeepFace)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1409.1556\" rel=\"nofollow\"\u003e2014/09 Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1409.4842\" rel=\"nofollow\"\u003e2014/09 Goint deeper with convolutions (GoogLeNet)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1411.4038\" rel=\"nofollow\"\u003e2014/11 Fully Convolutional Networks for Semantic Segmentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1505.04597\" rel=\"nofollow\"\u003e2015/05 U-Net: Convolutional Networks for Biomedical Image Segmentation (U-Net)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1512.03385\" rel=\"nofollow\"\u003e2015/12 Deep Residual Learning for Image Recognition (ResNet)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1704.04861\" rel=\"nofollow\"\u003e2017/04 MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (MobileNets)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1707.01083\" rel=\"nofollow\"\u003e2017/07 ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices (ShuffleNet)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"資料集-dataset\" class=\"anchor\" href=\"#%E8%B3%87%E6%96%99%E9%9B%86-dataset\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e資料集 Dataset\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://www.image-net.org/papers/imagenet_cvpr09.pdf\" rel=\"nofollow\"\u003e2009/06 ImageNet: A Large-Scale Hierarchical Image Database (ImageNet)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"物體偵測與切割-object-detection-and-segmentation\" class=\"anchor\" href=\"#%E7%89%A9%E9%AB%94%E5%81%B5%E6%B8%AC%E8%88%87%E5%88%87%E5%89%B2-object-detection-and-segmentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e物體偵測與切割 Object Detection and Segmentation\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1311.2524\" rel=\"nofollow\"\u003e2013/11 Rich feature hierarchies for accurate object detection and semantic segmentation (R-CNN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1312.6229\" rel=\"nofollow\"\u003e2013/12 OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks (OverFeat)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1504.08083\" rel=\"nofollow\"\u003e2015/04 Fast R-CNN\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1506.01497\" rel=\"nofollow\"\u003e2015/06 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (Faster R-CNN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1506.02640\" rel=\"nofollow\"\u003e2015/06 You Only Look Once: Unified, Real-Time Object Detection (YOLO)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1512.02325\" rel=\"nofollow\"\u003e2015/12 SSD: Single Shot MultiBox Detector (SSD)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1612.08242\" rel=\"nofollow\"\u003e2016/12 YOLO9000: Better, Faster, Stronger (YOLOv2)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1703.06870\" rel=\"nofollow\"\u003e2017/03 Mask R-CNN\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1804.02767\" rel=\"nofollow\"\u003e2018/04 YOLOv3: An Incremental Improvement (YOLOv3)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"生成模型-generative-models\" class=\"anchor\" href=\"#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B-generative-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e生成模型 Generative Models\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1406.2661\" rel=\"nofollow\"\u003e2014/06 Generative Adversarial Networks (GAN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1511.06434\" rel=\"nofollow\"\u003e2015/13 Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1701.07875\" rel=\"nofollow\"\u003e2017/01 Wasserstein GAN (WGAN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1703.10593\" rel=\"nofollow\"\u003e2017/03 Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"其他整理\" class=\"anchor\" href=\"#%E5%85%B6%E4%BB%96%E6%95%B4%E7%90%86\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cdiv\u003e其他整理\u003c/div\u003e\n\u003c/h2\u003e\n\u003cp\u003e這邊列出其他優質的資源整理網站 / Github Repo，供你繼續探索深度學習。\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"deep-learning-ocean\" class=\"anchor\" href=\"#deep-learning-ocean\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/osforscience/deep-learning-ocean\" rel=\"nofollow\"\u003edeep-learning-ocean\u003c/a\u003e\n\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e整理了不少深度學習資源，但最值得參考的是數據集以及論文的分類整理。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"待辦事項\" class=\"anchor\" href=\"#%E5%BE%85%E8%BE%A6%E4%BA%8B%E9%A0%85\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e待辦事項\u003c/h2\u003e\n\u003cp\u003e還有不少內容正在整理，以下是目前我們打算增加的一些項目：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e深度學習中英術語對照表\u003c/li\u003e\n\u003cli\u003e值得追蹤的業界 / 學界影響人物清單\u003c/li\u003e\n\u003cli\u003e無圖的資源列表版本\u003c/li\u003e\n\u003cli\u003e一些 Jupyter Notebook 範例\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e而我們也會持續將新資源加入如\u003ca href=\"#tools\"\u003e實用工具\u003c/a\u003e、\u003ca href=\"#blogs\"\u003e優質文章\u003c/a\u003e等列表裡頭。\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"如何貢獻\" class=\"anchor\" href=\"#%E5%A6%82%E4%BD%95%E8%B2%A2%E7%8D%BB\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e如何貢獻\u003c/h2\u003e\n\u003cp\u003e非常歡迎你一起加入改善這個 Repo，讓更多人有方向地學習 Deep Learning：）\u003c/p\u003e\n\u003cp\u003e如果你有\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e其他值得推薦的深度學習資源\u003c/li\u003e\n\u003cli\u003e針對此 Repo 內容的改善建議\u003c/li\u003e\n\u003cli\u003e其他任何你想得到的東西\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e都歡迎你\u003ca href=\"https://github.com/leemengtaiwan/deep-learning-resources/issues/new\" rel=\"nofollow\"\u003e提出新的 Issue\u003c/a\u003e 來讓我們知道。\u003c/p\u003e\n\u003cp\u003e如果是想增加新資源的話，只附上連結也是沒有問題的，謝謝！\u003c/p\u003e\n\u003c/body\u003e\u003c/html\u003e\n","project_id":164206566,"name":"deep-learning-resources","owner_login":"leemengtaiwan","description":"由淺入深的深度學習資源 Collection of deep learning materials for everyone","pushed_at":"2021-01-02T05:59:12.000Z","stargazers_count":401,"open_issues_count":0,"has_downloads":true,"archived":false,"license_key":"mit","homepage":"https://leemeng.tw/deep-learning-resources.html","private":false,"project_created_at":"2019-01-05T10:56:25.000Z","project_updated_at":"2021-01-13T17:15:53.000Z","language":null,"added_language_topic":false},"categories":[{"name":"Advertising","topicsCount":10},{"name":"Application Programming Interfaces","topicsCount":124},{"name":"Applications","topicsCount":192},{"name":"Artificial Intelligence","topicsCount":78},{"name":"Blockchain","topicsCount":73},{"name":"Build Tools","topicsCount":113},{"name":"Cloud Computing","topicsCount":80},{"name":"Code Quality","topicsCount":28},{"name":"Collaboration","topicsCount":32},{"name":"Command Line Interface","topicsCount":49},{"name":"Community","topicsCount":83},{"name":"Companies","topicsCount":60},{"name":"Compilers","topicsCount":63},{"name":"Computer Science","topicsCount":80},{"name":"Configuration Management","topicsCount":42},{"name":"Content Management","topicsCount":175},{"name":"Control Flow","topicsCount":213},{"name":"Data Formats","topicsCount":78},{"name":"Data Processing","topicsCount":276},{"name":"Data Storage","topicsCount":135},{"name":"Economics","topicsCount":64},{"name":"Frameworks","topicsCount":215},{"name":"Games","topicsCount":129},{"name":"Graphics","topicsCount":110},{"name":"Hardware","topicsCount":152},{"name":"Integrated Development Environments","topicsCount":49},{"name":"Learning Resources","topicsCount":166},{"name":"Legal","topicsCount":29},{"name":"Libraries","topicsCount":129},{"name":"Lists Of Projects","topicsCount":22},{"name":"Machine Learning","topicsCount":347},{"name":"Mapping","topicsCount":64},{"name":"Marketing","topicsCount":15},{"name":"Mathematics","topicsCount":55},{"name":"Media","topicsCount":239},{"name":"Messaging","topicsCount":98},{"name":"Networking","topicsCount":315},{"name":"Operating Systems","topicsCount":89},{"name":"Operations","topicsCount":121},{"name":"Package Managers","topicsCount":55},{"name":"Programming Languages","topicsCount":245},{"name":"Runtime Environments","topicsCount":100},{"name":"Science","topicsCount":42},{"name":"Security","topicsCount":396},{"name":"Social Media","topicsCount":27},{"name":"Software Architecture","topicsCount":72},{"name":"Software Development","topicsCount":72},{"name":"Software Performance","topicsCount":58},{"name":"Software Quality","topicsCount":133},{"name":"Text Editors","topicsCount":49},{"name":"Text Processing","topicsCount":136},{"name":"User Interface","topicsCount":330},{"name":"User Interface Components","topicsCount":514},{"name":"Version Control","topicsCount":30},{"name":"Virtualization","topicsCount":71},{"name":"Web Browsers","topicsCount":42},{"name":"Web Servers","topicsCount":26},{"name":"Web User Interface","topicsCount":210},{"name":"All Projects","topicsCount":0}],"advertisement":{"variation":"project","title":"Become A Software Engineer At Top Companies","description":"Identify your strengths with a free online coding quiz, and skip resume and recruiter screens at multiple companies at once.  It's free, confidential, includes a free flight and hotel, along with help to study to pass interviews and negotiate a high salary!","url":"https://triplebyte.com/a/PGNifkM/pt2"},"nonCommercial":false,"nonDerivative":false}</script>
      



    <!-- load synchronously before app bundle so adsbygoogle is always defined, google's new ad code doesn't seem to like this, at least for the initial review  -->
    <!-- try loading programatically and async with a callback to push the ads and see if it helps page load speed -->
    <!--script src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script-->
    <script src="/packs/root-bundle-1f53b6db74aa6b1bef19.js"></script>
  </body>
</html>
